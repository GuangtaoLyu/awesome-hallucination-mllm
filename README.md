# Hallucination in Vision-Language Models: Paper List

*Last updated: 2026-02-07 | Total papers: 152*


## 2026

1. **Where Does Vision Meet Language? Understanding and Refining Visual Fusion in MLLMs via Contrastive Attention**  
   Song et al.  
   [![arXiv](https://img.shields.io/badge/arXiv-2601.08151-b31b1b.svg)](https://arxiv.org/abs/2601.08151)

2. **Vision-Language Introspection: Mitigating Overconfident Hallucinations in MLLMs via Interpretable Bi-Causal Steering**  
   Liu et al.  
   [![arXiv](https://img.shields.io/badge/arXiv-2601.05159-b31b1b.svg)](https://arxiv.org/abs/2601.05159)

3. **VIB-Probe: Detecting and Mitigating Hallucinations in Vision-Language Models via Variational Information Bottleneck**  
   Zhang et al.  
   [![arXiv](https://img.shields.io/badge/arXiv-2601.05547-b31b1b.svg)](https://arxiv.org/abs/2601.05547)

4. **Seeing Right but Saying Wrong: Inter- and Intra-Layer Refinement in MLLMs without Training**  
   Song et al.  
   [![arXiv](https://img.shields.io/badge/arXiv-2601.07359-b31b1b.svg)](https://arxiv.org/abs/2601.07359)

5. **SDCD: Structure-Disrupted Contrastive Decoding for Mitigating Hallucinations in Large Vision-Language Models**  
   Xia et al.  
   [![arXiv](https://img.shields.io/badge/arXiv-2601.03500-b31b1b.svg)](https://arxiv.org/abs/2601.03500)

6. **Residual Decoding: Mitigating Hallucinations in Large Vision-Language Models via History-Aware Residual Guidance**  
   Chen et al.  
   [![arXiv](https://img.shields.io/badge/arXiv-2602.01047-b31b1b.svg)](https://arxiv.org/abs/2602.01047)

7. **KVSmooth: Mitigating Hallucination in Multi-modal Large Language Models through Key-Value Smoothing**  
   Jiang et al.  
   [![arXiv](https://img.shields.io/badge/arXiv-2602.04268-b31b1b.svg)](https://arxiv.org/abs/2602.04268)

8. **Countering the Over-Reliance Trap: Mitigating Object Hallucination for LVLMs via a Self-Validation Framework**  
   Liu et al.  
   [![arXiv](https://img.shields.io/badge/arXiv-2601.22451-b31b1b.svg)](https://arxiv.org/abs/2601.22451)

9. **CRoPS: A Training-Free Hallucination Mitigation Framework for Vision-Language Models**  
   Anand et al.  
   [![arXiv](https://img.shields.io/badge/arXiv-2601.00659-b31b1b.svg)](https://arxiv.org/abs/2601.00659)


## 2025

10. **Why LVLMs Are More Prone to Hallucinations in Longer Responses: The Role of Context**  
   Zheng et al.  
   [![arXiv](https://img.shields.io/badge/arXiv-2510.20229-b31b1b.svg)](https://arxiv.org/abs/2510.20229)

11. **When Semantics Mislead Vision: Mitigating Large Multimodal Models Hallucinations in Scene Text Spotting and Understanding**  
   Shu et al.  
   [![arXiv](https://img.shields.io/badge/arXiv-2506.05551-b31b1b.svg)](https://arxiv.org/abs/2506.05551)

12. **When Images Speak Louder: Mitigating Language Bias-induced Hallucinations in VLMs through Cross-Modal Guidance**  
   Cao et al.  
   [![arXiv](https://img.shields.io/badge/arXiv-2510.10466-b31b1b.svg)](https://arxiv.org/abs/2510.10466)

13. **Watch Closely: Mitigating Object Hallucinations in Large Vision-Language Models with Disentangled Decoding**  
   Ma et al.  
   [![arXiv](https://img.shields.io/badge/arXiv-2512.19070-b31b1b.svg)](https://arxiv.org/abs/2512.19070)

14. **Visual Evidence Prompting Mitigates Hallucinations in Large Vision-Language Models**  
   Li et al.  

15. **Visual Description Grounding Reduces Hallucinations and Boosts Reasoning in LVLMs**  
   Ghosh et al.  
   [![arXiv](https://img.shields.io/badge/arXiv-2405.15683-b31b1b.svg)](https://arxiv.org/abs/2405.15683)

16. **Visual Attention Never Fades: Selective Progressive Attention ReCalibration for Detailed Image Captioning in Multimodal Large Language Models**  
   Jung et al.  
   [![arXiv](https://img.shields.io/badge/arXiv-2502.01419-b31b1b.svg)](https://arxiv.org/abs/2502.01419)

17. **VaLiD: Mitigating the Hallucination of Large Vision Language Models by Visual Layer Fusion Contrastive Decoding**  
   Wang et al.  
   [![arXiv](https://img.shields.io/badge/arXiv-2411.15839-b31b1b.svg)](https://arxiv.org/abs/2411.15839)

18. **VISTA: Enhancing Vision-Text Alignment in MLLMs via Cross- Modal Mutual Information Maximization**  
   Li et al.  

19. **VEGAS: Mitigating Hallucinations in Large Vision-Language Models via Vision-Encoder Attention Guided Adaptive Steering**  
   Wang et al.  
   [![arXiv](https://img.shields.io/badge/arXiv-2512.12089-b31b1b.svg)](https://arxiv.org/abs/2512.12089)

20. **VASparse: Towards Efficient Visual Hallucination Mitigation via Visual-Aware Token Sparsification**  
   Zhuang et al.  

21. **Unveiling the Response of Large Vision-Language Models to Visually Absent Tokens**  
   Kim et al.  
   [![arXiv](https://img.shields.io/badge/arXiv-2509.03025-b31b1b.svg)](https://arxiv.org/abs/2509.03025)

22. **Understanding and Mitigating Hallucination in Large Vision-Language Models via Modular Attribution and Intervention**  
   Yang et al.  

23. **TruthPrInt: Mitigating LVLM Object Hallucination Via Latent Truthful-Guided Pre-Intervention**  
   Duan et al.  
   [![arXiv](https://img.shields.io/badge/arXiv-2503.10602-b31b1b.svg)](https://arxiv.org/abs/2503.10602)

24. **Toward Robust Hyper-Detailed Image Captioning: A Multiagent Approach and Dual Evaluation Metrics for Factuality and Coverage**  
   Lee et al.  
   [![arXiv](https://img.shields.io/badge/arXiv-2412.15484-b31b1b.svg)](https://arxiv.org/abs/2412.15484)

25. **To Sink or Not to Sink: Visual Information Pathways in Large Vision-Language Models**  
   Luo et al.  
   [![arXiv](https://img.shields.io/badge/arXiv-2510.08510-b31b1b.svg)](https://arxiv.org/abs/2510.08510)

26. **Through the Magnifying Glass: Adaptive Perception Magnification for Hallucination-Free VLM Decoding**  
   Mao et al.  
   [![arXiv](https://img.shields.io/badge/arXiv-2503.10183-b31b1b.svg)](https://arxiv.org/abs/2503.10183)

27. **TPC: Cross-Temporal Prediction Connection for Vision-Language Model Hallucination Reduction**  
   Wang et al.  
   [![arXiv](https://img.shields.io/badge/arXiv-2503.04457-b31b1b.svg)](https://arxiv.org/abs/2503.04457)

28. **TARS: MinMax Token-Adaptive Preference Strategy for MLLM Hallucination Reduction**  
   Zhang et al.  
   [![arXiv](https://img.shields.io/badge/arXiv-2507.21584-b31b1b.svg)](https://arxiv.org/abs/2507.21584)

29. **TARAC: Mitigating Hallucination in LVLMs via Temporal Attention Real-time Accumulative Connection**  
   Xie et al.  
   [![arXiv](https://img.shields.io/badge/arXiv-2504.04099-b31b1b.svg)](https://arxiv.org/abs/2504.04099)

30. **Suppressing VLM Hallucinations with Spectral Representation Filtering**  
   Ali et al.  
   [![arXiv](https://img.shields.io/badge/arXiv-2511.12220-b31b1b.svg)](https://arxiv.org/abs/2511.12220)

31. **Steering LVLMs via Sparse Autoencoder for Hallucination Mitigation**  
   Hua et al.  
   [![arXiv](https://img.shields.io/badge/arXiv-2505.16146-b31b1b.svg)](https://arxiv.org/abs/2505.16146)

32. **Shallow Focus, Deep Fixes: Enhancing Shallow Layers Vision Attention Sinks to Alleviate Hallucination in LVLMs**  
   Zhang et al.  

33. **Self-Introspective Decoding: Alleviating Hallucinations for Large Vision-Language Models**  
   Huo et al.  
   [![arXiv](https://img.shields.io/badge/arXiv-2408.02032-b31b1b.svg)](https://arxiv.org/abs/2408.02032)

34. **Self-Correcting Decoding with Generative Feedback for Mitigating Hallucinations in Large Vision-Language Models**  
   Zhang et al.  
   [![arXiv](https://img.shields.io/badge/arXiv-2502.06130-b31b1b.svg)](https://arxiv.org/abs/2502.06130)

35. **Self-Consistency as a Free Lunch: Reducing Hallucinations in Vision-Language Models via Self-Reflection**  
   Han et al.  
   [![arXiv](https://img.shields.io/badge/arXiv-2509.23236-b31b1b.svg)](https://arxiv.org/abs/2509.23236)

36. **Self-Augmented Visual Contrastive Decoding**  
   Im et al.  
   [![arXiv](https://img.shields.io/badge/arXiv-2510.13315-b31b1b.svg)](https://arxiv.org/abs/2510.13315)

37. **Seeing but Not Believing: Probing the Disconnect Between Visual Attention and Answer Correctness in VLMs**  
   Liu et al.  
   [![arXiv](https://img.shields.io/badge/arXiv-2510.17771-b31b1b.svg)](https://arxiv.org/abs/2510.17771)

38. **Seeing It or Not? Interpretable Vision-aware Latent Steering to Mitigate Object Hallucinations**  
   Chen et al.  
   [![arXiv](https://img.shields.io/badge/arXiv-2505.17812-b31b1b.svg)](https://arxiv.org/abs/2505.17812)

39. **Seeing Far and Clearly: Mitigating Hallucinations in MLLMs with Attention Causal Decoding**  
   Tang et al.  

40. **See What You Are Told: Visual Attention Sink in Large Multimodal Models**  
   Kang et al.  
   [![arXiv](https://img.shields.io/badge/arXiv-2503.03321-b31b1b.svg)](https://arxiv.org/abs/2503.03321)

41. **SECOND: Mitigating Perceptual Hallucination in Vision-Language Models via Selective and Contrastive Decoding**  
   Park et al.  
   [![arXiv](https://img.shields.io/badge/arXiv-2506.08391-b31b1b.svg)](https://arxiv.org/abs/2506.08391)

42. **SAVER: Mitigating Hallucinations in Large Vision-Language Models via Style-Aware Visual Early Revision**  
   Li et al.  
   [![arXiv](https://img.shields.io/badge/arXiv-2508.03177-b31b1b.svg)](https://arxiv.org/abs/2508.03177)

43. **SAVE: Sparse Autoencoder-Driven Visual Information Enhancement for Mitigating Object Hallucination**  
   Park et al.  
   [![arXiv](https://img.shields.io/badge/arXiv-2512.07730-b31b1b.svg)](https://arxiv.org/abs/2512.07730)

44. **Revisit What You See: Disclose Language Prior in Vision Tokens for LVLM Decoding**  
   Cho et al.  
   [![arXiv](https://img.shields.io/badge/arXiv-2506.09522-b31b1b.svg)](https://arxiv.org/abs/2506.09522)

45. **Revisit What You See: Disclose Language Prior in Vision Tokens for Efficient Guided Decoding of LVLMs**  
   Cho et al.  
   [![arXiv](https://img.shields.io/badge/arXiv-2506.09522-b31b1b.svg)](https://arxiv.org/abs/2506.09522)

46. **Reducing Hallucinations in Vision-Language Models via Latent Space Steering**  
   Liu et al.  

47. **Re-Align: Aligning Vision Language Models via Retrieval-Augmented Direct Preference Optimization**  
   Xing et al.  
   [![arXiv](https://img.shields.io/badge/arXiv-2502.13146-b31b1b.svg)](https://arxiv.org/abs/2502.13146)

48. **RAR: Reversing Visual Attention Re-sinking for Unlocking Potential in Multimodal Large Language Models**  
   Anonymous et al.  

49. **Qwen Look Again: Guiding Vision-Language Reasoning Models to Re-attention Visual Information**  
   Chu et al.  
   [![arXiv](https://img.shields.io/badge/arXiv-2505.23558-b31b1b.svg)](https://arxiv.org/abs/2505.23558)

50. **Poison as Cure: Visual Noise for Mitigating Object Hallucinations in LVMs**  
   Zhang et al.  
   [![arXiv](https://img.shields.io/badge/arXiv-2501.19164-b31b1b.svg)](https://arxiv.org/abs/2501.19164)

51. **PerturboLLaVA: Reducing Multimodal Hallucinations with Perturbative Visual Training**  
   Chen et al.  
   [![arXiv](https://img.shields.io/badge/arXiv-2503.06486-b31b1b.svg)](https://arxiv.org/abs/2503.06486)

52. **PerturboLLaVA: Reducing Multimodal Hallucinations with Perturbative Visual Training**  
   Chen et al.  
   [![arXiv](https://img.shields.io/badge/arXiv-2503.06486-b31b1b.svg)](https://arxiv.org/abs/2503.06486)

53. **Paying More Attention to Image: A Training-Free Method for Alleviating Hallucination in LVLMs**  
   Anonymous et al.  
   [Paper](https://link.springer.com/10.1007/978-3-031-73010-8_8)

54. **PAINT: Paying Attention to INformed Tokens to Mitigate Hallucination in Large Vision-Language Model**  
   Arif et al.  
   [![arXiv](https://img.shields.io/badge/arXiv-2501.12206-b31b1b.svg)](https://arxiv.org/abs/2501.12206)

55. **Octopus: Alleviating Hallucination via Dynamic Contrastive Decoding**  
   Suo et al.  
   [![arXiv](https://img.shields.io/badge/arXiv-2503.00361-b31b1b.svg)](https://arxiv.org/abs/2503.00361)

56. **ONLY: One-Layer Intervention Sufficiently Mitigates Hallucinations in Large Vision-Language Models**  
   Wan et al.  
   [![arXiv](https://img.shields.io/badge/arXiv-2507.00898-b31b1b.svg)](https://arxiv.org/abs/2507.00898)

57. **ONLY: One-Layer Intervention Sufficiently Mitigates Hallucinations in Large Vision-Language Models**  
   Wan et al.  
   [![arXiv](https://img.shields.io/badge/arXiv-2507.00898-b31b1b.svg)](https://arxiv.org/abs/2507.00898)

58. **Nullu: Mitigating Object Hallucinations in Large Vision-Language Models via HalluSpace Projection**  
   Yang et al.  

59. **Not All Tokens and Heads Are Equally Important: Dual-Level Attention Intervention for Hallucination Mitigation**  
   Tang et al.  
   [![arXiv](https://img.shields.io/badge/arXiv-2506.12609-b31b1b.svg)](https://arxiv.org/abs/2506.12609)

60. **MoLE: Decoding by Mixture of Layer Experts Alleviates Hallucination in Large Vision-Language Models**  
   Liang et al.  

61. **Mixture of Decoding: An Attention-Inspired Adaptive Decoding Strategy to Mitigate Hallucinations in Large Vision-Language Models**  
   Chen et al.  
   [![arXiv](https://img.shields.io/badge/arXiv-2505.17061-b31b1b.svg)](https://arxiv.org/abs/2505.17061)

62. **Mitigating Object Hallucinations via Sentence-Level Early Intervention**  
   Peng et al.  
   [![arXiv](https://img.shields.io/badge/arXiv-2507.12455-b31b1b.svg)](https://arxiv.org/abs/2507.12455)

63. **Mitigating Object Hallucinations in MLLMs via Multi-Frequency Perturbations**  
   Li et al.  
   [![arXiv](https://img.shields.io/badge/arXiv-2503.14895-b31b1b.svg)](https://arxiv.org/abs/2503.14895)

64. **Mitigating Object Hallucinations in Large Vision-Language Models with Assembly of Global and Local Attention**  
   An et al.  

65. **Mitigating Object Hallucinations in Large Vision-Language Models via Attention Calibration**  
   Zhu et al.  
   [![arXiv](https://img.shields.io/badge/arXiv-2502.01969-b31b1b.svg)](https://arxiv.org/abs/2502.01969)

66. **Mitigating Object Hallucinations in Large Vision-Language Models through Visual Contrastive Decoding**  
   Leng et al.  

67. **Mitigating Object Hallucination via Concentric Causal Attention**  
   Xing et al.  

68. **Mitigating Object Hallucination in Large Vision-Language Models via Image-Grounded Guidance**  
   Zhao et al.  
   [![arXiv](https://img.shields.io/badge/arXiv-2402.08680-b31b1b.svg)](https://arxiv.org/abs/2402.08680)

69. **Mitigating Modality Prior-Induced Hallucinations in Multimodal Large Language Models via Deciphering Attention Causality**  
   Zhou et al.  
   [![arXiv](https://img.shields.io/badge/arXiv-2410.04780-b31b1b.svg)](https://arxiv.org/abs/2410.04780)

70. **Mitigating Hallucinations in Multi-modal Large Language Models via Image Token Attention-Guided Decoding**  
   Xu et al.  

71. **Mitigating Hallucinations in Large Vision-Language Models via Reasoning Uncertainty-Guided Refinement**  
   Li et al.  
   [Paper](https://ieeexplore.ieee.org/document/11125489/)

72. **Mitigating Hallucinations in Large Vision-Language Models via Entity-Centric Multimodal Preference Optimization**  
   Wu et al.  
   [![arXiv](https://img.shields.io/badge/arXiv-2506.04039-b31b1b.svg)](https://arxiv.org/abs/2506.04039)

73. **Mitigating Hallucinations in Large Vision-Language Models by Adaptively Constraining Information Flow**  
   Bai et al.  

74. **Mitigating Hallucination in Multimodal LLMs with Layer Contrastive Decoding**  
   Tong et al.  
   [![arXiv](https://img.shields.io/badge/arXiv-2509.25177-b31b1b.svg)](https://arxiv.org/abs/2509.25177)

75. **Mitigating Hallucination in Large Vision-Language Models via Adaptive Attention Calibration**  
   Fazli et al.  
   [![arXiv](https://img.shields.io/badge/arXiv-2505.21472-b31b1b.svg)](https://arxiv.org/abs/2505.21472)

76. **Mitigating Hallucination for Large Vision Language Model by Inter-Modality Correlation Calibration Decoding**  
   Li et al.  
   [![arXiv](https://img.shields.io/badge/arXiv-2501.01926-b31b1b.svg)](https://arxiv.org/abs/2501.01926)

77. **Mitigating Attention Sinks and Massive Activations in Audio-Visual Speech Recognition with LLMs**  
   Anand et al.  
   [![arXiv](https://img.shields.io/badge/arXiv-2510.22603-b31b1b.svg)](https://arxiv.org/abs/2510.22603)

78. **Mitigate Language Priors in Large Vision-Language Models by Cross-Images Contrastive Decoding**  
   Zhao et al.  
   [![arXiv](https://img.shields.io/badge/arXiv-2505.10634-b31b1b.svg)](https://arxiv.org/abs/2505.10634)

79. **MaskCD: Mitigating LVLM Hallucinations by Image Head Masked Contrastive Decoding**  
   Deng et al.  
   [![arXiv](https://img.shields.io/badge/arXiv-2510.02790-b31b1b.svg)](https://arxiv.org/abs/2510.02790)

80. **MLLM can see? Dynamic Correction Decoding for Hallucination Mitigation**  
   Wang et al.  
   [![arXiv](https://img.shields.io/badge/arXiv-2410.11779-b31b1b.svg)](https://arxiv.org/abs/2410.11779)

81. **MINT: Mitigating Hallucinations in Large Vision-Language Models via Token Reduction**  
   Wang et al.  
   [![arXiv](https://img.shields.io/badge/arXiv-2502.00717-b31b1b.svg)](https://arxiv.org/abs/2502.00717)

82. **MIHBench: Benchmarking and Mitigating Multi-Image Hallucinations in Multimodal Large Language Models**  
   Li et al.  
   [![arXiv](https://img.shields.io/badge/arXiv-2508.00726-b31b1b.svg)](https://arxiv.org/abs/2508.00726)

83. **MCA-LLaVA: Manhattan Causal Attention for Reducing Hallucination in Large Vision-Language Models**  
   Zhao et al.  
   [![arXiv](https://img.shields.io/badge/arXiv-2507.09184-b31b1b.svg)](https://arxiv.org/abs/2507.09184)

84. **MAP: Mitigating Hallucinations in Large Vision-Language Models with Map-Level Attention Processing**  
   Li et al.  
   [![arXiv](https://img.shields.io/badge/arXiv-2508.01653-b31b1b.svg)](https://arxiv.org/abs/2508.01653)

85. **Look Twice Before You Answer: Memory-Space Visual Retracing for Hallucination Mitigation in Multimodal Large Language Models**  
   Zou et al.  
   [![arXiv](https://img.shields.io/badge/arXiv-2410.03577-b31b1b.svg)](https://arxiv.org/abs/2410.03577)

86. **Intervening Anchor Token: Decoding Strategy in Alleviating Hallucinations for MLLMs**  
   Tang et al.  

87. **Interpreting and Editing Vision-Language Representations to Mitigate Hallucinations**  
   Jiang et al.  

88. **Insight Over Sight: Exploring the Vision-Knowledge Conflicts in Multimodal LLMs**  
   Liu et al.  
   [![arXiv](https://img.shields.io/badge/arXiv-2410.08145-b31b1b.svg)](https://arxiv.org/abs/2410.08145)

89. **InEx: Hallucination Mitigation via Introspection and Cross-Modal Multi-Agent Collaboration**  
   Yang et al.  
   [![arXiv](https://img.shields.io/badge/arXiv-2512.02981-b31b1b.svg)](https://arxiv.org/abs/2512.02981)

90. **Improve Decoding Factuality by Token-wise Cross Layer Entropy of Large Language Models**  
   Wu et al.  

91. **INTER: Mitigating Hallucination in Large Vision-Language Models by Interaction Guidance Sampling**  
   Dong et al.  
   [![arXiv](https://img.shields.io/badge/arXiv-2507.05056-b31b1b.svg)](https://arxiv.org/abs/2507.05056)

92. **Hallucinatory Image Tokens: A Training-free EAZY Approach on Detecting and Mitigating Object Hallucinations in LVLMs**  
   Che et al.  
   [![arXiv](https://img.shields.io/badge/arXiv-2503.07772-b31b1b.svg)](https://arxiv.org/abs/2503.07772)

93. **Grounding Language with Vision: A Conditional Mutual Information Calibrated Decoding Strategy for Reducing Hallucinations in LVLMs**  
   Fang et al.  
   [![arXiv](https://img.shields.io/badge/arXiv-2505.19678-b31b1b.svg)](https://arxiv.org/abs/2505.19678)

94. **GHOST: Hallucination-Inducing Image Generation for Multimodal LLMs**  
   Parast et al.  
   [![arXiv](https://img.shields.io/badge/arXiv-2509.25178-b31b1b.svg)](https://arxiv.org/abs/2509.25178)

95. **Focus on What Matters: Enhancing Medical Vision-Language Models with Automatic Attention Alignment Tuning**  
   Chang et al.  
   [![arXiv](https://img.shields.io/badge/arXiv-2505.18503-b31b1b.svg)](https://arxiv.org/abs/2505.18503)

96. **Extracting Visual Facts from Intermediate Layers for Mitigating Hallucinations in Multimodal Large Language Models**  
   Zhou et al.  
   [![arXiv](https://img.shields.io/badge/arXiv-2507.15652-b31b1b.svg)](https://arxiv.org/abs/2507.15652)

97. **Enhancing Visual Reliance in Text Generation: A Bayesian Perspective on Mitigating Hallucination in Large Vision-Language Models**  
   Hu et al.  
   [![arXiv](https://img.shields.io/badge/arXiv-2505.19498-b31b1b.svg)](https://arxiv.org/abs/2505.19498)

98. **Energy-Guided Decoding for Object Hallucination Mitigation**  
   Liu et al.  
   [![arXiv](https://img.shields.io/badge/arXiv-2507.07731-b31b1b.svg)](https://arxiv.org/abs/2507.07731)

99. **Don't Miss the Forest for the Trees: Attentional Vision Calibration for Large Vision Language Models**  
   Woo et al.  
   [![arXiv](https://img.shields.io/badge/arXiv-2405.17820-b31b1b.svg)](https://arxiv.org/abs/2405.17820)

100. **Don't Deceive Me: Mitigating Gaslighting through Attention Reallocation in LMMs**  
   Jiao et al.  
   [![arXiv](https://img.shields.io/badge/arXiv-2504.09456-b31b1b.svg)](https://arxiv.org/abs/2504.09456)

101. **Devils in Middle Layers of Large Vision-Language Models: Interpreting, Detecting and Mitigating Object Hallucinations via Attention Lens**  
   Jiang et al.  
   [![arXiv](https://img.shields.io/badge/arXiv-2411.16724-b31b1b.svg)](https://arxiv.org/abs/2411.16724)

102. **Decoupling Contrastive Decoding: Robust Hallucination Mitigation in Multimodal Large Language Models**  
   Chen et al.  
   [![arXiv](https://img.shields.io/badge/arXiv-2504.08809-b31b1b.svg)](https://arxiv.org/abs/2504.08809)

103. **Damo: Decoding by Accumulating Activations Momentum for Mitigating Hallucinations in Vision-Language Models**  
   Wang et al.  

104. **D-LEAF: Localizing and Correcting Hallucinations in Multimodal LLMs via Layer-to-head Attention Diagnostics**  
   Yang et al.  
   [![arXiv](https://img.shields.io/badge/arXiv-2509.07864-b31b1b.svg)](https://arxiv.org/abs/2509.07864)

105. **Cross-Image Contrastive Decoding: Precise, Lossless Suppression of Language Priors in Large Vision-Language Models**  
   Zhao et al.  
   [![arXiv](https://img.shields.io/badge/arXiv-2505.10634-b31b1b.svg)](https://arxiv.org/abs/2505.10634)

106. **Cracking the Code of Hallucination in LVLMs with Vision-aware Head Divergence**  
   He et al.  
   [![arXiv](https://img.shields.io/badge/arXiv-2412.13949-b31b1b.svg)](https://arxiv.org/abs/2412.13949)

107. **Contrastive Decoding Reduces Hallucinations in Large Multilingual Machine Translation Models**  
   Waldendorf et al.  

108. **Conscious Gaze: Adaptive Attention Mechanisms for Hallucination Mitigation in Vision-Language Models**  
   Bu et al.  
   [![arXiv](https://img.shields.io/badge/arXiv-2512.05546-b31b1b.svg)](https://arxiv.org/abs/2512.05546)

109. **CoFi-Dec: Hallucination-Resistant Decoding via Coarse-to-Fine Generative Feedback in Large Vision-Language Models**  
   Cao et al.  
   [![arXiv](https://img.shields.io/badge/arXiv-2512.23453-b31b1b.svg)](https://arxiv.org/abs/2512.23453)

110. **Causally-Grounded Dual-Path Attention Intervention for Object Hallucination Mitigation in LVLMs**  
   Yu et al.  
   [![arXiv](https://img.shields.io/badge/arXiv-2511.09018-b31b1b.svg)](https://arxiv.org/abs/2511.09018)

111. **Causally-Grounded Dual-Path Attention Intervention for Object Hallucination Mitigation in LVLMs**  
   Yu et al.  
   [![arXiv](https://img.shields.io/badge/arXiv-2511.09018-b31b1b.svg)](https://arxiv.org/abs/2511.09018)

112. **Causal Tracing of Object Representations in Large Vision Language Models: Mechanistic Interpretability and Hallucination Mitigation**  
   Li et al.  
   [![arXiv](https://img.shields.io/badge/arXiv-2511.05923-b31b1b.svg)](https://arxiv.org/abs/2511.05923)

113. **Capturing Gaze Shifts for Guidance: Cross-Modal Fusion Enhancement for VLM Hallucination Mitigation**  
   Qi et al.  
   [![arXiv](https://img.shields.io/badge/arXiv-2510.22067-b31b1b.svg)](https://arxiv.org/abs/2510.22067)

114. **CLAIM: Mitigating Multilingual Object Hallucination in Large Vision-Language Models with Cross-Lingual Attention Intervention**  
   Ye et al.  
   [![arXiv](https://img.shields.io/badge/arXiv-2506.11073-b31b1b.svg)](https://arxiv.org/abs/2506.11073)

115. **CAI: Caption-Sensitive Attention Intervention for Mitigating Object Hallucination in Large Vision-Language Models**  
   Li et al.  
   [![arXiv](https://img.shields.io/badge/arXiv-2506.23590-b31b1b.svg)](https://arxiv.org/abs/2506.23590)

116. **Beyond Single Models: Mitigating Multimodal Hallucinations via Adaptive Token Ensemble Decoding**  
   Li et al.  
   [![arXiv](https://img.shields.io/badge/arXiv-2510.18321-b31b1b.svg)](https://arxiv.org/abs/2510.18321)

117. **Benchmarking and Bridging Emotion Conflicts for Multimodal Emotion Reasoning**  
   Han et al.  
   [![arXiv](https://img.shields.io/badge/arXiv-2508.01181-b31b1b.svg)](https://arxiv.org/abs/2508.01181)

118. **BIMA: Bijective Maximum Likelihood Learning Approach to Hallucination Prediction and Mitigation in Large Vision-Language Models**  
   Tran et al.  

119. **Attention Hijackers: Detect and Disentangle Attention Hijacking in LVLMs for Hallucination Mitigation**  
   Chen et al.  
   [![arXiv](https://img.shields.io/badge/arXiv-2503.08216-b31b1b.svg)](https://arxiv.org/abs/2503.08216)

120. **Analyzing and Mitigating Object Hallucination: A Training Bias Perspective**  
   Li et al.  
   [![arXiv](https://img.shields.io/badge/arXiv-2508.04567-b31b1b.svg)](https://arxiv.org/abs/2508.04567)

121. **Alleviating Hallucination in Large Vision-Language Models with Active Retrieval Augmentation**  
   Qu et al.  

122. **Adaptive Residual-Update Steering for Low-Overhead Hallucination Mitigation in Large Vision Language Models**  
   Zou et al.  
   [![arXiv](https://img.shields.io/badge/arXiv-2511.10292-b31b1b.svg)](https://arxiv.org/abs/2511.10292)

123. **Activation Steering Decoding: Mitigating Hallucination in Large Vision-Language Models through Bidirectional Hidden State Intervention**  
   Su et al.  

124. **AVCD: Mitigating Hallucinations in Audio-Visual Large Language Models through Contrastive Decoding**  
   Jung et al.  
   [![arXiv](https://img.shields.io/badge/arXiv-2505.20862-b31b1b.svg)](https://arxiv.org/abs/2505.20862)

125. **ASCD: Attention-Steerable Contrastive Decoding for Reducing Hallucination in MLLM**  
   Wang et al.  
   [![arXiv](https://img.shields.io/badge/arXiv-2506.14766-b31b1b.svg)](https://arxiv.org/abs/2506.14766)

126. **A Survey of Multimodal Hallucination Evaluation and Detection**  
   Chen et al.  
   [![arXiv](https://img.shields.io/badge/arXiv-2507.19024-b31b1b.svg)](https://arxiv.org/abs/2507.19024)


## 2024

127. **Verb Mirage: Unveiling and Assessing Verb Concept Hallucinations in Multimodal Large Language Models**  
   Wang et al.  
   [![arXiv](https://img.shields.io/badge/arXiv-2412.04939-b31b1b.svg)](https://arxiv.org/abs/2412.04939)

128. **VaLiD: Mitigating the Hallucination of Large Vision Language Models by Visual Layer Fusion Contrastive Decoding**  
   Wang et al.  
   [![arXiv](https://img.shields.io/badge/arXiv-2411.15839-b31b1b.svg)](https://arxiv.org/abs/2411.15839)

129. **VACoDe: Visual Augmented Contrastive Decoding**  
   Kim et al.  
   [![arXiv](https://img.shields.io/badge/arXiv-2408.05337-b31b1b.svg)](https://arxiv.org/abs/2408.05337)

130. **Unraveling Cross-Modality Knowledge Conflict in Large Vision-Language Models**  
   Zhu et al.  
   [![arXiv](https://img.shields.io/badge/arXiv-2410.03659-b31b1b.svg)](https://arxiv.org/abs/2410.03659)

131. **Seeing the Image: Prioritizing Visual Correlation by Contrastive Alignment**  
   Xiao et al.  
   [![arXiv](https://img.shields.io/badge/arXiv-2405.17871-b31b1b.svg)](https://arxiv.org/abs/2405.17871)

132. **Seeing Clearly by Layer Two: Enhancing Attention Heads to Alleviate Hallucination in LVLMs**  
   Zhang et al.  
   [![arXiv](https://img.shields.io/badge/arXiv-2411.09968-b31b1b.svg)](https://arxiv.org/abs/2411.09968)

133. **RITUAL: Random Image Transformations as a Universal Anti-hallucination Lever in Large Vision Language Models**  
   Woo et al.  
   [![arXiv](https://img.shields.io/badge/arXiv-2405.17821-b31b1b.svg)](https://arxiv.org/abs/2405.17821)

134. **OPERA: Alleviating Hallucination in Multi-Modal Large Language Models via Over-Trust Penalty and Retrospection-Allocation**  
   Huang et al.  
   [Paper](https://ieeexplore.ieee.org/document/10655465/)

135. **Multi-Modal Hallucination Control by Visual Information Grounding**  
   Favero et al.  
   [Paper](https://ieeexplore.ieee.org/document/10655750/)

136. **Mitigating Multilingual Hallucination in Large Vision-Language Models**  
   Qu et al.  
   [![arXiv](https://img.shields.io/badge/arXiv-2408.00550-b31b1b.svg)](https://arxiv.org/abs/2408.00550)

137. **Mitigating Hallucinations in Large Vision-Language Models with Instruction Contrastive Decoding**  
   Wang et al.  
   [![arXiv](https://img.shields.io/badge/arXiv-2403.18715-b31b1b.svg)](https://arxiv.org/abs/2403.18715)

138. **Layer Importance and Hallucination Analysis in Large Language Models via Enhanced Activation Variance-Sparsity**  
   Song et al.  
   [![arXiv](https://img.shields.io/badge/arXiv-2411.10069-b31b1b.svg)](https://arxiv.org/abs/2411.10069)

139. **Instructive Decoding: Instruction-Tuned Large Language Models are Self-Refiner from Noisy Instructions**  
   Kim et al.  
   [![arXiv](https://img.shields.io/badge/arXiv-2311.00233-b31b1b.svg)](https://arxiv.org/abs/2311.00233)

140. **In-Context Sharpness as Alerts: An Inner Representation Perspective for Hallucination Mitigation**  
   Chen et al.  
   [![arXiv](https://img.shields.io/badge/arXiv-2403.01548-b31b1b.svg)](https://arxiv.org/abs/2403.01548)

141. **IBD: Alleviating Hallucinations in Large Vision-Language Models via Image-Biased Decoding**  
   Zhu et al.  
   [![arXiv](https://img.shields.io/badge/arXiv-2402.18476-b31b1b.svg)](https://arxiv.org/abs/2402.18476)

142. **HALC: Object Hallucination Reduction via Adaptive Focal-Contrast Decoding**  
   Chen et al.  
   [![arXiv](https://img.shields.io/badge/arXiv-2403.00425-b31b1b.svg)](https://arxiv.org/abs/2403.00425)

143. **DoLa: Decoding by Contrasting Layers Improves Factuality in Large Language Models**  
   Chuang et al.  
   [![arXiv](https://img.shields.io/badge/arXiv-2309.03883-b31b1b.svg)](https://arxiv.org/abs/2309.03883)

144. **DAMRO: Dive into the Attention Mechanism of LVLM to Reduce Object Hallucination**  
   Gong et al.  
   [Paper](https://aclanthology.org/2024.emnlp-main.439)

145. **Contrastive Region Guidance: Improving Grounding in Vision-Language Models without Training**  
   Wan et al.  
   [![arXiv](https://img.shields.io/badge/arXiv-2403.02325-b31b1b.svg)](https://arxiv.org/abs/2403.02325)

146. **ConVis: Contrastive Decoding with Hallucination Visualization for Mitigating Hallucinations in Multimodal Large Language Models**  
   Park et al.  
   [![arXiv](https://img.shields.io/badge/arXiv-2408.13906-b31b1b.svg)](https://arxiv.org/abs/2408.13906)

147. **CODE: Contrasting Self-generated Description to Combat Hallucination in Large Multi-modal Models**  
   Kim et al.  
   [![arXiv](https://img.shields.io/badge/arXiv-2406.01920-b31b1b.svg)](https://arxiv.org/abs/2406.01920)

148. **Analyzing and Mitigating Object Hallucination in Large Vision-Language Models**  
   Zhou et al.  
   [![arXiv](https://img.shields.io/badge/arXiv-2310.00754-b31b1b.svg)](https://arxiv.org/abs/2310.00754)

149. **Alleviating Hallucinations in Large Vision-Language Models through Hallucination-Induced Optimization**  
   Chen et al.  
   [![arXiv](https://img.shields.io/badge/arXiv-2405.15356-b31b1b.svg)](https://arxiv.org/abs/2405.15356)


## 2023

150. **Mitigating Object Hallucinations in Large Vision-Language Models through Visual Contrastive Decoding**  
   Leng et al.  
   [![arXiv](https://img.shields.io/badge/arXiv-2311.16922-b31b1b.svg)](https://arxiv.org/abs/2311.16922)

151. **Evaluating Object Hallucination in Large Vision-Language Models**  
   Li et al.  
   [![arXiv](https://img.shields.io/badge/arXiv-2305.10355-b31b1b.svg)](https://arxiv.org/abs/2305.10355)

152. **Contrastive Decoding: Open-ended Text Generation as Optimization**  
   Li et al.  
   [![arXiv](https://img.shields.io/badge/arXiv-2210.15097-b31b1b.svg)](https://arxiv.org/abs/2210.15097)

