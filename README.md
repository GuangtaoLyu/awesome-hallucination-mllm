# Hallucination in Vision-Language Models: Paper List

*Last updated: 2026-02-07 | Total papers: 152*


## 2026
1. **Where Does Vision Meet Language? Understanding and Refining Visual Fusion in MLLMs via Contrastive Attention**  ,   Song et al.  ,   [![arXiv](https://img.shields.io/badge/arXiv-2601.08151-b31b1b.svg)](https://arxiv.org/abs/2601.08151)

2. **Vision-Language Introspection: Mitigating Overconfident Hallucinations in MLLMs via Interpretable Bi-Causal Steering**  ,   Liu et al.  ,   [![arXiv](https://img.shields.io/badge/arXiv-2601.05159-b31b1b.svg)](https://arxiv.org/abs/2601.05159)

3. **VIB-Probe: Detecting and Mitigating Hallucinations in Vision-Language Models via Variational Information Bottleneck**  ,   Zhang et al.  ,   [![arXiv](https://img.shields.io/badge/arXiv-2601.05547-b31b1b.svg)](https://arxiv.org/abs/2601.05547)

4. **Seeing Right but Saying Wrong: Inter- and Intra-Layer Refinement in MLLMs without Training**  ,   Song et al.  ,   [![arXiv](https://img.shields.io/badge/arXiv-2601.07359-b31b1b.svg)](https://arxiv.org/abs/2601.07359)

5. **SDCD: Structure-Disrupted Contrastive Decoding for Mitigating Hallucinations in Large Vision-Language Models**  ,   Xia et al.  ,   [![arXiv](https://img.shields.io/badge/arXiv-2601.03500-b31b1b.svg)](https://arxiv.org/abs/2601.03500)

6. **Residual Decoding: Mitigating Hallucinations in Large Vision-Language Models via History-Aware Residual Guidance**  ,   Chen et al.  ,   [![arXiv](https://img.shields.io/badge/arXiv-2602.01047-b31b1b.svg)](https://arxiv.org/abs/2602.01047)

7. **KVSmooth: Mitigating Hallucination in Multi-modal Large Language Models through Key-Value Smoothing**  ,   Jiang et al.  ,   [![arXiv](https://img.shields.io/badge/arXiv-2602.04268-b31b1b.svg)](https://arxiv.org/abs/2602.04268)

8. **Countering the Over-Reliance Trap: Mitigating Object Hallucination for LVLMs via a Self-Validation Framework**  ,   Liu et al.  ,   [![arXiv](https://img.shields.io/badge/arXiv-2601.22451-b31b1b.svg)](https://arxiv.org/abs/2601.22451)

9. **CRoPS: A Training-Free Hallucination Mitigation Framework for Vision-Language Models**  ,   Anand et al.  ,   [![arXiv](https://img.shields.io/badge/arXiv-2601.00659-b31b1b.svg)](https://arxiv.org/abs/2601.00659)


## 2025
10. **Why LVLMs Are More Prone to Hallucinations in Longer Responses: The Role of Context**  ,   Zheng et al.  ,   [![arXiv](https://img.shields.io/badge/arXiv-2510.20229-b31b1b.svg)](https://arxiv.org/abs/2510.20229)

11. **When Semantics Mislead Vision: Mitigating Large Multimodal Models Hallucinations in Scene Text Spotting and Understanding**  ,   Shu et al.  ,   [![arXiv](https://img.shields.io/badge/arXiv-2506.05551-b31b1b.svg)](https://arxiv.org/abs/2506.05551)

12. **When Images Speak Louder: Mitigating Language Bias-induced Hallucinations in VLMs through Cross-Modal Guidance**  ,   Cao et al.  ,   [![arXiv](https://img.shields.io/badge/arXiv-2510.10466-b31b1b.svg)](https://arxiv.org/abs/2510.10466)

13. **Watch Closely: Mitigating Object Hallucinations in Large Vision-Language Models with Disentangled Decoding**  ,   Ma et al.  ,   [![arXiv](https://img.shields.io/badge/arXiv-2512.19070-b31b1b.svg)](https://arxiv.org/abs/2512.19070)

14. **Visual Description Grounding Reduces Hallucinations and Boosts Reasoning in LVLMs**  ,   Ghosh et al.  ,   [![arXiv](https://img.shields.io/badge/arXiv-2405.15683-b31b1b.svg)](https://arxiv.org/abs/2405.15683)

15. **Visual Attention Never Fades: Selective Progressive Attention ReCalibration for Detailed Image Captioning in Multimodal Large Language Models**  ,   Jung et al.  ,   [![arXiv](https://img.shields.io/badge/arXiv-2502.01419-b31b1b.svg)](https://arxiv.org/abs/2502.01419)

16. **VaLiD: Mitigating the Hallucination of Large Vision Language Models by Visual Layer Fusion Contrastive Decoding**  ,   Wang et al.  ,   [![arXiv](https://img.shields.io/badge/arXiv-2411.15839-b31b1b.svg)](https://arxiv.org/abs/2411.15839)

17. **VEGAS: Mitigating Hallucinations in Large Vision-Language Models via Vision-Encoder Attention Guided Adaptive Steering**  ,   Wang et al.  ,   [![arXiv](https://img.shields.io/badge/arXiv-2512.12089-b31b1b.svg)](https://arxiv.org/abs/2512.12089)

18. **Unveiling the Response of Large Vision-Language Models to Visually Absent Tokens**  ,   Kim et al.  ,   [![arXiv](https://img.shields.io/badge/arXiv-2509.03025-b31b1b.svg)](https://arxiv.org/abs/2509.03025)

19. **Understanding and Mitigating Hallucination in Large Vision-Language Models via Modular Attribution and Intervention**  ,   Yang et al.  ,
20. **TruthPrInt: Mitigating LVLM Object Hallucination Via Latent Truthful-Guided Pre-Intervention**  ,   Duan et al.  ,   [![arXiv](https://img.shields.io/badge/arXiv-2503.10602-b31b1b.svg)](https://arxiv.org/abs/2503.10602)

21. **Toward Robust Hyper-Detailed Image Captioning: A Multiagent Approach and Dual Evaluation Metrics for Factuality and Coverage**  ,   Lee et al.  ,   [![arXiv](https://img.shields.io/badge/arXiv-2412.15484-b31b1b.svg)](https://arxiv.org/abs/2412.15484)

22. **To Sink or Not to Sink: Visual Information Pathways in Large Vision-Language Models**  ,   Luo et al.  ,   [![arXiv](https://img.shields.io/badge/arXiv-2510.08510-b31b1b.svg)](https://arxiv.org/abs/2510.08510)

23. **Through the Magnifying Glass: Adaptive Perception Magnification for Hallucination-Free VLM Decoding**  ,   Mao et al.  ,   [![arXiv](https://img.shields.io/badge/arXiv-2503.10183-b31b1b.svg)](https://arxiv.org/abs/2503.10183)

24. **TPC: Cross-Temporal Prediction Connection for Vision-Language Model Hallucination Reduction**  ,   Wang et al.  ,   [![arXiv](https://img.shields.io/badge/arXiv-2503.04457-b31b1b.svg)](https://arxiv.org/abs/2503.04457)

25. **TARS: MinMax Token-Adaptive Preference Strategy for MLLM Hallucination Reduction**  ,   Zhang et al.  ,   [![arXiv](https://img.shields.io/badge/arXiv-2507.21584-b31b1b.svg)](https://arxiv.org/abs/2507.21584)

26. **TARAC: Mitigating Hallucination in LVLMs via Temporal Attention Real-time Accumulative Connection**  ,   Xie et al.  ,   [![arXiv](https://img.shields.io/badge/arXiv-2504.04099-b31b1b.svg)](https://arxiv.org/abs/2504.04099)

27. **Suppressing VLM Hallucinations with Spectral Representation Filtering**  ,   Ali et al.  ,   [![arXiv](https://img.shields.io/badge/arXiv-2511.12220-b31b1b.svg)](https://arxiv.org/abs/2511.12220)

28. **Steering LVLMs via Sparse Autoencoder for Hallucination Mitigation**  ,   Hua et al.  ,   [![arXiv](https://img.shields.io/badge/arXiv-2505.16146-b31b1b.svg)](https://arxiv.org/abs/2505.16146)

29. **Self-Introspective Decoding: Alleviating Hallucinations for Large Vision-Language Models**  ,   Huo et al.  ,   [![arXiv](https://img.shields.io/badge/arXiv-2408.02032-b31b1b.svg)](https://arxiv.org/abs/2408.02032)

30. **Self-Correcting Decoding with Generative Feedback for Mitigating Hallucinations in Large Vision-Language Models**  ,   Zhang et al.  ,   [![arXiv](https://img.shields.io/badge/arXiv-2502.06130-b31b1b.svg)](https://arxiv.org/abs/2502.06130)

31. **Self-Consistency as a Free Lunch: Reducing Hallucinations in Vision-Language Models via Self-Reflection**  ,   Han et al.  ,   [![arXiv](https://img.shields.io/badge/arXiv-2509.23236-b31b1b.svg)](https://arxiv.org/abs/2509.23236)

32. **Self-Augmented Visual Contrastive Decoding**  ,   Im et al.  ,   [![arXiv](https://img.shields.io/badge/arXiv-2510.13315-b31b1b.svg)](https://arxiv.org/abs/2510.13315)

33. **Seeing but Not Believing: Probing the Disconnect Between Visual Attention and Answer Correctness in VLMs**  ,   Liu et al.  ,   [![arXiv](https://img.shields.io/badge/arXiv-2510.17771-b31b1b.svg)](https://arxiv.org/abs/2510.17771)

34. **Seeing It or Not? Interpretable Vision-aware Latent Steering to Mitigate Object Hallucinations**  ,   Chen et al.  ,   [![arXiv](https://img.shields.io/badge/arXiv-2505.17812-b31b1b.svg)](https://arxiv.org/abs/2505.17812)

35. **See What You Are Told: Visual Attention Sink in Large Multimodal Models**  ,   Kang et al.  ,   [![arXiv](https://img.shields.io/badge/arXiv-2503.03321-b31b1b.svg)](https://arxiv.org/abs/2503.03321)

36. **SECOND: Mitigating Perceptual Hallucination in Vision-Language Models via Selective and Contrastive Decoding**  ,   Park et al.  ,   [![arXiv](https://img.shields.io/badge/arXiv-2506.08391-b31b1b.svg)](https://arxiv.org/abs/2506.08391)

37. **SAVER: Mitigating Hallucinations in Large Vision-Language Models via Style-Aware Visual Early Revision**  ,   Li et al.  ,   [![arXiv](https://img.shields.io/badge/arXiv-2508.03177-b31b1b.svg)](https://arxiv.org/abs/2508.03177)

38. **SAVE: Sparse Autoencoder-Driven Visual Information Enhancement for Mitigating Object Hallucination**  ,   Park et al.  ,   [![arXiv](https://img.shields.io/badge/arXiv-2512.07730-b31b1b.svg)](https://arxiv.org/abs/2512.07730)

39. **Revisit What You See: Disclose Language Prior in Vision Tokens for LVLM Decoding**  ,   Cho et al.  ,   [![arXiv](https://img.shields.io/badge/arXiv-2506.09522-b31b1b.svg)](https://arxiv.org/abs/2506.09522)

40. **Revisit What You See: Disclose Language Prior in Vision Tokens for Efficient Guided Decoding of LVLMs**  ,   Cho et al.  ,   [![arXiv](https://img.shields.io/badge/arXiv-2506.09522-b31b1b.svg)](https://arxiv.org/abs/2506.09522)

41. **Reducing Hallucinations in Vision-Language Models via Latent Space Steering**  ,   Liu et al.  ,
42. **Re-Align: Aligning Vision Language Models via Retrieval-Augmented Direct Preference Optimization**  ,   Xing et al.  ,   [![arXiv](https://img.shields.io/badge/arXiv-2502.13146-b31b1b.svg)](https://arxiv.org/abs/2502.13146)

43. **RAR: Reversing Visual Attention Re-sinking for Unlocking Potential in Multimodal Large Language Models**  ,   Anonymous et al.  ,
44. **Qwen Look Again: Guiding Vision-Language Reasoning Models to Re-attention Visual Information**  ,   Chu et al.  ,   [![arXiv](https://img.shields.io/badge/arXiv-2505.23558-b31b1b.svg)](https://arxiv.org/abs/2505.23558)

45. **Poison as Cure: Visual Noise for Mitigating Object Hallucinations in LVMs**  ,   Zhang et al.  ,   [![arXiv](https://img.shields.io/badge/arXiv-2501.19164-b31b1b.svg)](https://arxiv.org/abs/2501.19164)

46. **PerturboLLaVA: Reducing Multimodal Hallucinations with Perturbative Visual Training**  ,   Chen et al.  ,   [![arXiv](https://img.shields.io/badge/arXiv-2503.06486-b31b1b.svg)](https://arxiv.org/abs/2503.06486)

47. **PerturboLLaVA: Reducing Multimodal Hallucinations with Perturbative Visual Training**  ,   Chen et al.  ,   [![arXiv](https://img.shields.io/badge/arXiv-2503.06486-b31b1b.svg)](https://arxiv.org/abs/2503.06486)

48. **Paying More Attention to Image: A Training-Free Method for Alleviating Hallucination in LVLMs**  ,   Anonymous et al.  ,   [Paper](https://link.springer.com/10.1007/978-3-031-73010-8_8)
49. **PAINT: Paying Attention to INformed Tokens to Mitigate Hallucination in Large Vision-Language Model**  ,   Arif et al.  ,   [![arXiv](https://img.shields.io/badge/arXiv-2501.12206-b31b1b.svg)](https://arxiv.org/abs/2501.12206)

50. **Octopus: Alleviating Hallucination via Dynamic Contrastive Decoding**  ,   Suo et al.  ,   [![arXiv](https://img.shields.io/badge/arXiv-2503.00361-b31b1b.svg)](https://arxiv.org/abs/2503.00361)

51. **ONLY: One-Layer Intervention Sufficiently Mitigates Hallucinations in Large Vision-Language Models**  ,   Wan et al.  ,   [![arXiv](https://img.shields.io/badge/arXiv-2507.00898-b31b1b.svg)](https://arxiv.org/abs/2507.00898)

52. **ONLY: One-Layer Intervention Sufficiently Mitigates Hallucinations in Large Vision-Language Models**  ,   Wan et al.  ,   [![arXiv](https://img.shields.io/badge/arXiv-2507.00898-b31b1b.svg)](https://arxiv.org/abs/2507.00898)

53. **Not All Tokens and Heads Are Equally Important: Dual-Level Attention Intervention for Hallucination Mitigation**  ,   Tang et al.  ,   [![arXiv](https://img.shields.io/badge/arXiv-2506.12609-b31b1b.svg)](https://arxiv.org/abs/2506.12609)

54. **Mixture of Decoding: An Attention-Inspired Adaptive Decoding Strategy to Mitigate Hallucinations in Large Vision-Language Models**  ,   Chen et al.  ,   [![arXiv](https://img.shields.io/badge/arXiv-2505.17061-b31b1b.svg)](https://arxiv.org/abs/2505.17061)

55. **Mitigating Object Hallucinations via Sentence-Level Early Intervention**  ,   Peng et al.  ,   [![arXiv](https://img.shields.io/badge/arXiv-2507.12455-b31b1b.svg)](https://arxiv.org/abs/2507.12455)

56. **Mitigating Object Hallucinations in MLLMs via Multi-Frequency Perturbations**  ,   Li et al.  ,   [![arXiv](https://img.shields.io/badge/arXiv-2503.14895-b31b1b.svg)](https://arxiv.org/abs/2503.14895)

57. **Mitigating Object Hallucinations in Large Vision-Language Models via Attention Calibration**  ,   Zhu et al.  ,   [![arXiv](https://img.shields.io/badge/arXiv-2502.01969-b31b1b.svg)](https://arxiv.org/abs/2502.01969)

58. **Mitigating Object Hallucination in Large Vision-Language Models via Image-Grounded Guidance**  ,   Zhao et al.  ,   [![arXiv](https://img.shields.io/badge/arXiv-2402.08680-b31b1b.svg)](https://arxiv.org/abs/2402.08680)

59. **Mitigating Modality Prior-Induced Hallucinations in Multimodal Large Language Models via Deciphering Attention Causality**  ,   Zhou et al.  ,   [![arXiv](https://img.shields.io/badge/arXiv-2410.04780-b31b1b.svg)](https://arxiv.org/abs/2410.04780)

60. **Mitigating Hallucinations in Large Vision-Language Models via Reasoning Uncertainty-Guided Refinement**  ,   Li et al.  ,   [Paper](https://ieeexplore.ieee.org/document/11125489/)
61. **Mitigating Hallucinations in Large Vision-Language Models via Entity-Centric Multimodal Preference Optimization**  ,   Wu et al.  ,   [![arXiv](https://img.shields.io/badge/arXiv-2506.04039-b31b1b.svg)](https://arxiv.org/abs/2506.04039)

62. **Mitigating Hallucination in Multimodal LLMs with Layer Contrastive Decoding**  ,   Tong et al.  ,   [![arXiv](https://img.shields.io/badge/arXiv-2509.25177-b31b1b.svg)](https://arxiv.org/abs/2509.25177)

63. **Mitigating Hallucination in Large Vision-Language Models via Adaptive Attention Calibration**  ,   Fazli et al.  ,   [![arXiv](https://img.shields.io/badge/arXiv-2505.21472-b31b1b.svg)](https://arxiv.org/abs/2505.21472)

64. **Mitigating Hallucination for Large Vision Language Model by Inter-Modality Correlation Calibration Decoding**  ,   Li et al.  ,   [![arXiv](https://img.shields.io/badge/arXiv-2501.01926-b31b1b.svg)](https://arxiv.org/abs/2501.01926)

65. **Mitigating Attention Sinks and Massive Activations in Audio-Visual Speech Recognition with LLMs**  ,   Anand et al.  ,   [![arXiv](https://img.shields.io/badge/arXiv-2510.22603-b31b1b.svg)](https://arxiv.org/abs/2510.22603)

66. **Mitigate Language Priors in Large Vision-Language Models by Cross-Images Contrastive Decoding**  ,   Zhao et al.  ,   [![arXiv](https://img.shields.io/badge/arXiv-2505.10634-b31b1b.svg)](https://arxiv.org/abs/2505.10634)

67. **MaskCD: Mitigating LVLM Hallucinations by Image Head Masked Contrastive Decoding**  ,   Deng et al.  ,   [![arXiv](https://img.shields.io/badge/arXiv-2510.02790-b31b1b.svg)](https://arxiv.org/abs/2510.02790)

68. **MLLM can see? Dynamic Correction Decoding for Hallucination Mitigation**  ,   Wang et al.  ,   [![arXiv](https://img.shields.io/badge/arXiv-2410.11779-b31b1b.svg)](https://arxiv.org/abs/2410.11779)

69. **MINT: Mitigating Hallucinations in Large Vision-Language Models via Token Reduction**  ,   Wang et al.  ,   [![arXiv](https://img.shields.io/badge/arXiv-2502.00717-b31b1b.svg)](https://arxiv.org/abs/2502.00717)

70. **MIHBench: Benchmarking and Mitigating Multi-Image Hallucinations in Multimodal Large Language Models**  ,   Li et al.  ,   [![arXiv](https://img.shields.io/badge/arXiv-2508.00726-b31b1b.svg)](https://arxiv.org/abs/2508.00726)

71. **MCA-LLaVA: Manhattan Causal Attention for Reducing Hallucination in Large Vision-Language Models**  ,   Zhao et al.  ,   [![arXiv](https://img.shields.io/badge/arXiv-2507.09184-b31b1b.svg)](https://arxiv.org/abs/2507.09184)

72. **MAP: Mitigating Hallucinations in Large Vision-Language Models with Map-Level Attention Processing**  ,   Li et al.  ,   [![arXiv](https://img.shields.io/badge/arXiv-2508.01653-b31b1b.svg)](https://arxiv.org/abs/2508.01653)

73. **Look Twice Before You Answer: Memory-Space Visual Retracing for Hallucination Mitigation in Multimodal Large Language Models**  ,   Zou et al.  ,   [![arXiv](https://img.shields.io/badge/arXiv-2410.03577-b31b1b.svg)](https://arxiv.org/abs/2410.03577)

74. **Intervening Anchor Token: Decoding Strategy in Alleviating Hallucinations for MLLMs**  ,   Tang et al.  ,
75. **Interpreting and Editing Vision-Language Representations to Mitigate Hallucinations**  ,   Jiang et al.  ,
76. **Insight Over Sight: Exploring the Vision-Knowledge Conflicts in Multimodal LLMs**  ,   Liu et al.  ,   [![arXiv](https://img.shields.io/badge/arXiv-2410.08145-b31b1b.svg)](https://arxiv.org/abs/2410.08145)

77. **InEx: Hallucination Mitigation via Introspection and Cross-Modal Multi-Agent Collaboration**  ,   Yang et al.  ,   [![arXiv](https://img.shields.io/badge/arXiv-2512.02981-b31b1b.svg)](https://arxiv.org/abs/2512.02981)

78. **INTER: Mitigating Hallucination in Large Vision-Language Models by Interaction Guidance Sampling**  ,   Dong et al.  ,   [![arXiv](https://img.shields.io/badge/arXiv-2507.05056-b31b1b.svg)](https://arxiv.org/abs/2507.05056)

79. **Hallucinatory Image Tokens: A Training-free EAZY Approach on Detecting and Mitigating Object Hallucinations in LVLMs**  ,   Che et al.  ,   [![arXiv](https://img.shields.io/badge/arXiv-2503.07772-b31b1b.svg)](https://arxiv.org/abs/2503.07772)

80. **Grounding Language with Vision: A Conditional Mutual Information Calibrated Decoding Strategy for Reducing Hallucinations in LVLMs**  ,   Fang et al.  ,   [![arXiv](https://img.shields.io/badge/arXiv-2505.19678-b31b1b.svg)](https://arxiv.org/abs/2505.19678)

81. **GHOST: Hallucination-Inducing Image Generation for Multimodal LLMs**  ,   Parast et al.  ,   [![arXiv](https://img.shields.io/badge/arXiv-2509.25178-b31b1b.svg)](https://arxiv.org/abs/2509.25178)

82. **Focus on What Matters: Enhancing Medical Vision-Language Models with Automatic Attention Alignment Tuning**  ,   Chang et al.  ,   [![arXiv](https://img.shields.io/badge/arXiv-2505.18503-b31b1b.svg)](https://arxiv.org/abs/2505.18503)

83. **Extracting Visual Facts from Intermediate Layers for Mitigating Hallucinations in Multimodal Large Language Models**  ,   Zhou et al.  ,   [![arXiv](https://img.shields.io/badge/arXiv-2507.15652-b31b1b.svg)](https://arxiv.org/abs/2507.15652)

84. **Enhancing Visual Reliance in Text Generation: A Bayesian Perspective on Mitigating Hallucination in Large Vision-Language Models**  ,   Hu et al.  ,   [![arXiv](https://img.shields.io/badge/arXiv-2505.19498-b31b1b.svg)](https://arxiv.org/abs/2505.19498)

85. **Energy-Guided Decoding for Object Hallucination Mitigation**  ,   Liu et al.  ,   [![arXiv](https://img.shields.io/badge/arXiv-2507.07731-b31b1b.svg)](https://arxiv.org/abs/2507.07731)

86. **Don't Miss the Forest for the Trees: Attentional Vision Calibration for Large Vision Language Models**  ,   Woo et al.  ,   [![arXiv](https://img.shields.io/badge/arXiv-2405.17820-b31b1b.svg)](https://arxiv.org/abs/2405.17820)

87. **Don't Deceive Me: Mitigating Gaslighting through Attention Reallocation in LMMs**  ,   Jiao et al.  ,   [![arXiv](https://img.shields.io/badge/arXiv-2504.09456-b31b1b.svg)](https://arxiv.org/abs/2504.09456)

88. **Devils in Middle Layers of Large Vision-Language Models: Interpreting, Detecting and Mitigating Object Hallucinations via Attention Lens**  ,   Jiang et al.  ,   [![arXiv](https://img.shields.io/badge/arXiv-2411.16724-b31b1b.svg)](https://arxiv.org/abs/2411.16724)

89. **Decoupling Contrastive Decoding: Robust Hallucination Mitigation in Multimodal Large Language Models**  ,   Chen et al.  ,   [![arXiv](https://img.shields.io/badge/arXiv-2504.08809-b31b1b.svg)](https://arxiv.org/abs/2504.08809)

90. **Damo: Decoding by Accumulating Activations Momentum for Mitigating Hallucinations in Vision-Language Models**  ,   Wang et al.  ,
91. **D-LEAF: Localizing and Correcting Hallucinations in Multimodal LLMs via Layer-to-head Attention Diagnostics**  ,   Yang et al.  ,   [![arXiv](https://img.shields.io/badge/arXiv-2509.07864-b31b1b.svg)](https://arxiv.org/abs/2509.07864)

92. **Cross-Image Contrastive Decoding: Precise, Lossless Suppression of Language Priors in Large Vision-Language Models**  ,   Zhao et al.  ,   [![arXiv](https://img.shields.io/badge/arXiv-2505.10634-b31b1b.svg)](https://arxiv.org/abs/2505.10634)

93. **Cracking the Code of Hallucination in LVLMs with Vision-aware Head Divergence**  ,   He et al.  ,   [![arXiv](https://img.shields.io/badge/arXiv-2412.13949-b31b1b.svg)](https://arxiv.org/abs/2412.13949)

94. **Conscious Gaze: Adaptive Attention Mechanisms for Hallucination Mitigation in Vision-Language Models**  ,   Bu et al.  ,   [![arXiv](https://img.shields.io/badge/arXiv-2512.05546-b31b1b.svg)](https://arxiv.org/abs/2512.05546)

95. **CoFi-Dec: Hallucination-Resistant Decoding via Coarse-to-Fine Generative Feedback in Large Vision-Language Models**  ,   Cao et al.  ,   [![arXiv](https://img.shields.io/badge/arXiv-2512.23453-b31b1b.svg)](https://arxiv.org/abs/2512.23453)

96. **Causally-Grounded Dual-Path Attention Intervention for Object Hallucination Mitigation in LVLMs**  ,   Yu et al.  ,   [![arXiv](https://img.shields.io/badge/arXiv-2511.09018-b31b1b.svg)](https://arxiv.org/abs/2511.09018)

97. **Causally-Grounded Dual-Path Attention Intervention for Object Hallucination Mitigation in LVLMs**  ,   Yu et al.  ,   [![arXiv](https://img.shields.io/badge/arXiv-2511.09018-b31b1b.svg)](https://arxiv.org/abs/2511.09018)

98. **Causal Tracing of Object Representations in Large Vision Language Models: Mechanistic Interpretability and Hallucination Mitigation**  ,   Li et al.  ,   [![arXiv](https://img.shields.io/badge/arXiv-2511.05923-b31b1b.svg)](https://arxiv.org/abs/2511.05923)

99. **Capturing Gaze Shifts for Guidance: Cross-Modal Fusion Enhancement for VLM Hallucination Mitigation**  ,   Qi et al.  ,   [![arXiv](https://img.shields.io/badge/arXiv-2510.22067-b31b1b.svg)](https://arxiv.org/abs/2510.22067)

100. **CLAIM: Mitigating Multilingual Object Hallucination in Large Vision-Language Models with Cross-Lingual Attention Intervention**  ,   Ye et al.  ,   [![arXiv](https://img.shields.io/badge/arXiv-2506.11073-b31b1b.svg)](https://arxiv.org/abs/2506.11073)

101. **CAI: Caption-Sensitive Attention Intervention for Mitigating Object Hallucination in Large Vision-Language Models**  ,   Li et al.  ,   [![arXiv](https://img.shields.io/badge/arXiv-2506.23590-b31b1b.svg)](https://arxiv.org/abs/2506.23590)

102. **Beyond Single Models: Mitigating Multimodal Hallucinations via Adaptive Token Ensemble Decoding**  ,   Li et al.  ,   [![arXiv](https://img.shields.io/badge/arXiv-2510.18321-b31b1b.svg)](https://arxiv.org/abs/2510.18321)

103. **Benchmarking and Bridging Emotion Conflicts for Multimodal Emotion Reasoning**  ,   Han et al.  ,   [![arXiv](https://img.shields.io/badge/arXiv-2508.01181-b31b1b.svg)](https://arxiv.org/abs/2508.01181)

104. **Attention Hijackers: Detect and Disentangle Attention Hijacking in LVLMs for Hallucination Mitigation**  ,   Chen et al.  ,   [![arXiv](https://img.shields.io/badge/arXiv-2503.08216-b31b1b.svg)](https://arxiv.org/abs/2503.08216)

105. **Analyzing and Mitigating Object Hallucination: A Training Bias Perspective**  ,   Li et al.  ,   [![arXiv](https://img.shields.io/badge/arXiv-2508.04567-b31b1b.svg)](https://arxiv.org/abs/2508.04567)

106. **Adaptive Residual-Update Steering for Low-Overhead Hallucination Mitigation in Large Vision Language Models**  ,   Zou et al.  ,   [![arXiv](https://img.shields.io/badge/arXiv-2511.10292-b31b1b.svg)](https://arxiv.org/abs/2511.10292)

107. **AVCD: Mitigating Hallucinations in Audio-Visual Large Language Models through Contrastive Decoding**  ,   Jung et al.  ,   [![arXiv](https://img.shields.io/badge/arXiv-2505.20862-b31b1b.svg)](https://arxiv.org/abs/2505.20862)

108. **ASCD: Attention-Steerable Contrastive Decoding for Reducing Hallucination in MLLM**  ,   Wang et al.  ,   [![arXiv](https://img.shields.io/badge/arXiv-2506.14766-b31b1b.svg)](https://arxiv.org/abs/2506.14766)

109. **A Survey of Multimodal Hallucination Evaluation and Detection**  ,   Chen et al.  ,   [![arXiv](https://img.shields.io/badge/arXiv-2507.19024-b31b1b.svg)](https://arxiv.org/abs/2507.19024)


## 2024
110. **Visual Evidence Prompting Mitigates Hallucinations in Large Vision-Language Models**  ,   Li et al.  ,
111. **Verb Mirage: Unveiling and Assessing Verb Concept Hallucinations in Multimodal Large Language Models**  ,   Wang et al.  ,   [![arXiv](https://img.shields.io/badge/arXiv-2412.04939-b31b1b.svg)](https://arxiv.org/abs/2412.04939)

112. **VaLiD: Mitigating the Hallucination of Large Vision Language Models by Visual Layer Fusion Contrastive Decoding**  ,   Wang et al.  ,   [![arXiv](https://img.shields.io/badge/arXiv-2411.15839-b31b1b.svg)](https://arxiv.org/abs/2411.15839)

113. **VISTA: Enhancing Vision-Text Alignment in MLLMs via Cross- Modal Mutual Information Maximization**  ,   Li et al.  ,
114. **VASparse: Towards Efficient Visual Hallucination Mitigation via Visual-Aware Token Sparsification**  ,   Zhuang et al.  ,
115. **VACoDe: Visual Augmented Contrastive Decoding**  ,   Kim et al.  ,   [![arXiv](https://img.shields.io/badge/arXiv-2408.05337-b31b1b.svg)](https://arxiv.org/abs/2408.05337)

116. **Unraveling Cross-Modality Knowledge Conflict in Large Vision-Language Models**  ,   Zhu et al.  ,   [![arXiv](https://img.shields.io/badge/arXiv-2410.03659-b31b1b.svg)](https://arxiv.org/abs/2410.03659)

117. **Shallow Focus, Deep Fixes: Enhancing Shallow Layers Vision Attention Sinks to Alleviate Hallucination in LVLMs**  ,   Zhang et al.  ,
118. **Seeing the Image: Prioritizing Visual Correlation by Contrastive Alignment**  ,   Xiao et al.  ,   [![arXiv](https://img.shields.io/badge/arXiv-2405.17871-b31b1b.svg)](https://arxiv.org/abs/2405.17871)

119. **Seeing Far and Clearly: Mitigating Hallucinations in MLLMs with Attention Causal Decoding**  ,   Tang et al.  ,
120. **Seeing Clearly by Layer Two: Enhancing Attention Heads to Alleviate Hallucination in LVLMs**  ,   Zhang et al.  ,   [![arXiv](https://img.shields.io/badge/arXiv-2411.09968-b31b1b.svg)](https://arxiv.org/abs/2411.09968)

121. **RITUAL: Random Image Transformations as a Universal Anti-hallucination Lever in Large Vision Language Models**  ,   Woo et al.  ,   [![arXiv](https://img.shields.io/badge/arXiv-2405.17821-b31b1b.svg)](https://arxiv.org/abs/2405.17821)

122. **OPERA: Alleviating Hallucination in Multi-Modal Large Language Models via Over-Trust Penalty and Retrospection-Allocation**  ,   Huang et al.  ,   [Paper](https://ieeexplore.ieee.org/document/10655465/)
123. **Nullu: Mitigating Object Hallucinations in Large Vision-Language Models via HalluSpace Projection**  ,   Yang et al.  ,
124. **Multi-Modal Hallucination Control by Visual Information Grounding**  ,   Favero et al.  ,   [Paper](https://ieeexplore.ieee.org/document/10655750/)
125. **MoLE: Decoding by Mixture of Layer Experts Alleviates Hallucination in Large Vision-Language Models**  ,   Liang et al.  ,
126. **Mitigating Object Hallucinations in Large Vision-Language Models with Assembly of Global and Local Attention**  ,   An et al.  ,
127. **Mitigating Object Hallucinations in Large Vision-Language Models through Visual Contrastive Decoding**  ,   Leng et al.  ,
128. **Mitigating Object Hallucination via Concentric Causal Attention**  ,   Xing et al.  ,
129. **Mitigating Multilingual Hallucination in Large Vision-Language Models**  ,   Qu et al.  ,   [![arXiv](https://img.shields.io/badge/arXiv-2408.00550-b31b1b.svg)](https://arxiv.org/abs/2408.00550)

130. **Mitigating Hallucinations in Multi-modal Large Language Models via Image Token Attention-Guided Decoding**  ,   Xu et al.  ,
131. **Mitigating Hallucinations in Large Vision-Language Models with Instruction Contrastive Decoding**  ,   Wang et al.  ,   [![arXiv](https://img.shields.io/badge/arXiv-2403.18715-b31b1b.svg)](https://arxiv.org/abs/2403.18715)

132. **Mitigating Hallucinations in Large Vision-Language Models by Adaptively Constraining Information Flow**  ,   Bai et al.  ,
133. **Layer Importance and Hallucination Analysis in Large Language Models via Enhanced Activation Variance-Sparsity**  ,   Song et al.  ,   [![arXiv](https://img.shields.io/badge/arXiv-2411.10069-b31b1b.svg)](https://arxiv.org/abs/2411.10069)

134. **Instructive Decoding: Instruction-Tuned Large Language Models are Self-Refiner from Noisy Instructions**  ,   Kim et al.  ,   [![arXiv](https://img.shields.io/badge/arXiv-2311.00233-b31b1b.svg)](https://arxiv.org/abs/2311.00233)

135. **In-Context Sharpness as Alerts: An Inner Representation Perspective for Hallucination Mitigation**  ,   Chen et al.  ,   [![arXiv](https://img.shields.io/badge/arXiv-2403.01548-b31b1b.svg)](https://arxiv.org/abs/2403.01548)

136. **Improve Decoding Factuality by Token-wise Cross Layer Entropy of Large Language Models**  ,   Wu et al.  ,
137. **IBD: Alleviating Hallucinations in Large Vision-Language Models via Image-Biased Decoding**  ,   Zhu et al.  ,   [![arXiv](https://img.shields.io/badge/arXiv-2402.18476-b31b1b.svg)](https://arxiv.org/abs/2402.18476)

138. **HALC: Object Hallucination Reduction via Adaptive Focal-Contrast Decoding**  ,   Chen et al.  ,   [![arXiv](https://img.shields.io/badge/arXiv-2403.00425-b31b1b.svg)](https://arxiv.org/abs/2403.00425)

139. **DoLa: Decoding by Contrasting Layers Improves Factuality in Large Language Models**  ,   Chuang et al.  ,   [![arXiv](https://img.shields.io/badge/arXiv-2309.03883-b31b1b.svg)](https://arxiv.org/abs/2309.03883)

140. **DAMRO: Dive into the Attention Mechanism of LVLM to Reduce Object Hallucination**  ,   Gong et al.  ,   [Paper](https://aclanthology.org/2024.emnlp-main.439)
141. **Contrastive Region Guidance: Improving Grounding in Vision-Language Models without Training**  ,   Wan et al.  ,   [![arXiv](https://img.shields.io/badge/arXiv-2403.02325-b31b1b.svg)](https://arxiv.org/abs/2403.02325)

142. **Contrastive Decoding Reduces Hallucinations in Large Multilingual Machine Translation Models**  ,   Waldendorf et al.  ,
143. **ConVis: Contrastive Decoding with Hallucination Visualization for Mitigating Hallucinations in Multimodal Large Language Models**  ,   Park et al.  ,   [![arXiv](https://img.shields.io/badge/arXiv-2408.13906-b31b1b.svg)](https://arxiv.org/abs/2408.13906)

144. **CODE: Contrasting Self-generated Description to Combat Hallucination in Large Multi-modal Models**  ,   Kim et al.  ,   [![arXiv](https://img.shields.io/badge/arXiv-2406.01920-b31b1b.svg)](https://arxiv.org/abs/2406.01920)

145. **BIMA: Bijective Maximum Likelihood Learning Approach to Hallucination Prediction and Mitigation in Large Vision-Language Models**  ,   Tran et al.  ,
146. **Analyzing and Mitigating Object Hallucination in Large Vision-Language Models**  ,   Zhou et al.  ,   [![arXiv](https://img.shields.io/badge/arXiv-2310.00754-b31b1b.svg)](https://arxiv.org/abs/2310.00754)

147. **Alleviating Hallucinations in Large Vision-Language Models through Hallucination-Induced Optimization**  ,   Chen et al.  ,   [![arXiv](https://img.shields.io/badge/arXiv-2405.15356-b31b1b.svg)](https://arxiv.org/abs/2405.15356)

148. **Alleviating Hallucination in Large Vision-Language Models with Active Retrieval Augmentation**  ,   Qu et al.  ,
149. **Activation Steering Decoding: Mitigating Hallucination in Large Vision-Language Models through Bidirectional Hidden State Intervention**  ,   Su et al.  ,

## 2023
150. **Mitigating Object Hallucinations in Large Vision-Language Models through Visual Contrastive Decoding**  ,   Leng et al.  ,   [![arXiv](https://img.shields.io/badge/arXiv-2311.16922-b31b1b.svg)](https://arxiv.org/abs/2311.16922)

151. **Evaluating Object Hallucination in Large Vision-Language Models**  ,   Li et al.  ,   [![arXiv](https://img.shields.io/badge/arXiv-2305.10355-b31b1b.svg)](https://arxiv.org/abs/2305.10355)

152. **Contrastive Decoding: Open-ended Text Generation as Optimization**  ,   Li et al.  ,   [![arXiv](https://img.shields.io/badge/arXiv-2210.15097-b31b1b.svg)](https://arxiv.org/abs/2210.15097)

