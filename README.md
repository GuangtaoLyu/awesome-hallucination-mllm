# Hallucination in Vision-Language Models: Paper List

*Last updated: 2026-02-07 | Total papers: 152*


## 2026

1. **Where Does Vision Meet Language? Understanding and Refining Visual Fusion in MLLMs via Contrastive Attention**
   Song, Li, Yu
   [![arXiv](https://img.shields.io/badge/arXiv-2601.08151-b31b1b.svg)](https://arxiv.org/abs/2601.08151)

2. **Vision-Language Introspection: Mitigating Overconfident Hallucinations in MLLMs via Interpretable Bi-Causal Steering**
   Liu, Yang, Fang et al.
   [![arXiv](https://img.shields.io/badge/arXiv-2601.05159-b31b1b.svg)](https://arxiv.org/abs/2601.05159)

3. **VIB-Probe: Detecting and Mitigating Hallucinations in Vision-Language Models via Variational Information Bottleneck**
   Zhang, Wu, Wang et al.
   [![arXiv](https://img.shields.io/badge/arXiv-2601.05547-b31b1b.svg)](https://arxiv.org/abs/2601.05547)

4. **Seeing Right but Saying Wrong: Inter- and Intra-Layer Refinement in MLLMs without Training**
   Song, Li, Yu
   [![arXiv](https://img.shields.io/badge/arXiv-2601.07359-b31b1b.svg)](https://arxiv.org/abs/2601.07359)

5. **SDCD: Structure-Disrupted Contrastive Decoding for Mitigating Hallucinations in Large Vision-Language Models**
   Xia, Wang, Li
   [![arXiv](https://img.shields.io/badge/arXiv-2601.03500-b31b1b.svg)](https://arxiv.org/abs/2601.03500)

6. **Residual Decoding: Mitigating Hallucinations in Large Vision-Language Models via History-Aware Residual Guidance**
   Chen, Chu, Qiu et al.
   [![arXiv](https://img.shields.io/badge/arXiv-2602.01047-b31b1b.svg)](https://arxiv.org/abs/2602.01047)

7. **KVSmooth: Mitigating Hallucination in Multi-modal Large Language Models through Key-Value Smoothing**
   Jiang, Chen, Zhang et al.
   [![arXiv](https://img.shields.io/badge/arXiv-2602.04268-b31b1b.svg)](https://arxiv.org/abs/2602.04268)

8. **Countering the Over-Reliance Trap: Mitigating Object Hallucination for LVLMs via a Self-Validation Framework**
   Liu, Wen, Lan et al.
   [![arXiv](https://img.shields.io/badge/arXiv-2601.22451-b31b1b.svg)](https://arxiv.org/abs/2601.22451)

9. **CRoPS: A Training-Free Hallucination Mitigation Framework for Vision-Language Models**
   Anand, Jha, Bamba et al.
   [![arXiv](https://img.shields.io/badge/arXiv-2601.00659-b31b1b.svg)](https://arxiv.org/abs/2601.00659)


## 2025

10. **Why LVLMs Are More Prone to Hallucinations in Longer Responses: The Role of Context**
   Zheng, Qian, Tang et al.
   [![arXiv](https://img.shields.io/badge/arXiv-2510.20229-b31b1b.svg)](https://arxiv.org/abs/2510.20229)

11. **When Semantics Mislead Vision: Mitigating Large Multimodal Models Hallucinations in Scene Text Spotting and Understanding**
   Shu, Lin, Liu et al.
   [![arXiv](https://img.shields.io/badge/arXiv-2506.05551-b31b1b.svg)](https://arxiv.org/abs/2506.05551)

12. **When Images Speak Louder: Mitigating Language Bias-induced Hallucinations in VLMs through Cross-Modal Guidance**
   Cao, Chen, Wang et al.
   [![arXiv](https://img.shields.io/badge/arXiv-2510.10466-b31b1b.svg)](https://arxiv.org/abs/2510.10466)

13. **Watch Closely: Mitigating Object Hallucinations in Large Vision-Language Models with Disentangled Decoding**
   Ma, Yan, Zhang et al.
   [![arXiv](https://img.shields.io/badge/arXiv-2512.19070-b31b1b.svg)](https://arxiv.org/abs/2512.19070)

14. **Visual Description Grounding Reduces Hallucinations and Boosts Reasoning in LVLMs**
   Ghosh, Evuru, Kumar et al.
   [![arXiv](https://img.shields.io/badge/arXiv-2405.15683-b31b1b.svg)](https://arxiv.org/abs/2405.15683)

15. **Visual Attention Never Fades: Selective Progressive Attention ReCalibration for Detailed Image Captioning in Multimodal Large Language Models**
   Jung, Lee, Kim et al.
   [![arXiv](https://img.shields.io/badge/arXiv-2502.01419-b31b1b.svg)](https://arxiv.org/abs/2502.01419)

16. **VaLiD: Mitigating the Hallucination of Large Vision Language Models by Visual Layer Fusion Contrastive Decoding**
   Wang, Gao, Sang
   [![arXiv](https://img.shields.io/badge/arXiv-2411.15839-b31b1b.svg)](https://arxiv.org/abs/2411.15839)

17. **VEGAS: Mitigating Hallucinations in Large Vision-Language Models via Vision-Encoder Attention Guided Adaptive Steering**
   Wang, Xu, Xia et al.
   [![arXiv](https://img.shields.io/badge/arXiv-2512.12089-b31b1b.svg)](https://arxiv.org/abs/2512.12089)

18. **Unveiling the Response of Large Vision-Language Models to Visually Absent Tokens**
   Kim, Ryu, Park et al.
   [![arXiv](https://img.shields.io/badge/arXiv-2509.03025-b31b1b.svg)](https://arxiv.org/abs/2509.03025)

19. **Understanding and Mitigating Hallucination in Large Vision-Language Models via Modular Attribution and Intervention**
   Yang, Li, Cao et al.

20. **TruthPrInt: Mitigating LVLM Object Hallucination Via Latent Truthful-Guided Pre-Intervention**
   Duan, Kong, Cheng et al.
   [![arXiv](https://img.shields.io/badge/arXiv-2503.10602-b31b1b.svg)](https://arxiv.org/abs/2503.10602)

21. **Toward Robust Hyper-Detailed Image Captioning: A Multiagent Approach and Dual Evaluation Metrics for Factuality and Coverage**
   Lee, Yoon, Bui et al.
   [![arXiv](https://img.shields.io/badge/arXiv-2412.15484-b31b1b.svg)](https://arxiv.org/abs/2412.15484)

22. **To Sink or Not to Sink: Visual Information Pathways in Large Vision-Language Models**
   Luo, Fan, Wang et al.
   [![arXiv](https://img.shields.io/badge/arXiv-2510.08510-b31b1b.svg)](https://arxiv.org/abs/2510.08510)

23. **Through the Magnifying Glass: Adaptive Perception Magnification for Hallucination-Free VLM Decoding**
   Mao, Zhang, Cai
   [![arXiv](https://img.shields.io/badge/arXiv-2503.10183-b31b1b.svg)](https://arxiv.org/abs/2503.10183)

24. **TPC: Cross-Temporal Prediction Connection for Vision-Language Model Hallucination Reduction**
   Wang, Fu, Zhou
   [![arXiv](https://img.shields.io/badge/arXiv-2503.04457-b31b1b.svg)](https://arxiv.org/abs/2503.04457)

25. **TARS: MinMax Token-Adaptive Preference Strategy for MLLM Hallucination Reduction**
   Zhang, Tao, Luo et al.
   [![arXiv](https://img.shields.io/badge/arXiv-2507.21584-b31b1b.svg)](https://arxiv.org/abs/2507.21584)

26. **TARAC: Mitigating Hallucination in LVLMs via Temporal Attention Real-time Accumulative Connection**
   Xie, Liu, Jiang et al.
   [![arXiv](https://img.shields.io/badge/arXiv-2504.04099-b31b1b.svg)](https://arxiv.org/abs/2504.04099)

27. **Suppressing VLM Hallucinations with Spectral Representation Filtering**
   Ali, Zoabi, Wolf
   [![arXiv](https://img.shields.io/badge/arXiv-2511.12220-b31b1b.svg)](https://arxiv.org/abs/2511.12220)

28. **Steering LVLMs via Sparse Autoencoder for Hallucination Mitigation**
   Hua, He, Yao et al.
   [![arXiv](https://img.shields.io/badge/arXiv-2505.16146-b31b1b.svg)](https://arxiv.org/abs/2505.16146)

29. **Self-Introspective Decoding: Alleviating Hallucinations for Large Vision-Language Models**
   Huo, Xu, Zhang et al.
   [![arXiv](https://img.shields.io/badge/arXiv-2408.02032-b31b1b.svg)](https://arxiv.org/abs/2408.02032)

30. **Self-Correcting Decoding with Generative Feedback for Mitigating Hallucinations in Large Vision-Language Models**
   Zhang, Wan, Kan et al.
   [![arXiv](https://img.shields.io/badge/arXiv-2502.06130-b31b1b.svg)](https://arxiv.org/abs/2502.06130)

31. **Self-Consistency as a Free Lunch: Reducing Hallucinations in Vision-Language Models via Self-Reflection**
   Han, Hao, Zhou et al.
   [![arXiv](https://img.shields.io/badge/arXiv-2509.23236-b31b1b.svg)](https://arxiv.org/abs/2509.23236)

32. **Self-Augmented Visual Contrastive Decoding**
   Im, Ali, Gupta
   [![arXiv](https://img.shields.io/badge/arXiv-2510.13315-b31b1b.svg)](https://arxiv.org/abs/2510.13315)

33. **Seeing but Not Believing: Probing the Disconnect Between Visual Attention and Answer Correctness in VLMs**
   Liu, Chen, Liu et al.
   [![arXiv](https://img.shields.io/badge/arXiv-2510.17771-b31b1b.svg)](https://arxiv.org/abs/2510.17771)

34. **Seeing It or Not? Interpretable Vision-aware Latent Steering to Mitigate Object Hallucinations**
   Chen, Zheng, Yang et al.
   [![arXiv](https://img.shields.io/badge/arXiv-2505.17812-b31b1b.svg)](https://arxiv.org/abs/2505.17812)

35. **See What You Are Told: Visual Attention Sink in Large Multimodal Models**
   Kang, Kim, Kim et al.
   [![arXiv](https://img.shields.io/badge/arXiv-2503.03321-b31b1b.svg)](https://arxiv.org/abs/2503.03321)

36. **SECOND: Mitigating Perceptual Hallucination in Vision-Language Models via Selective and Contrastive Decoding**
   Park, Kim, Kim et al.
   [![arXiv](https://img.shields.io/badge/arXiv-2506.08391-b31b1b.svg)](https://arxiv.org/abs/2506.08391)

37. **SAVER: Mitigating Hallucinations in Large Vision-Language Models via Style-Aware Visual Early Revision**
   Li, Kong, Yu et al.
   [![arXiv](https://img.shields.io/badge/arXiv-2508.03177-b31b1b.svg)](https://arxiv.org/abs/2508.03177)

38. **SAVE: Sparse Autoencoder-Driven Visual Information Enhancement for Mitigating Object Hallucination**
   Park, Yoo, Mok et al.
   [![arXiv](https://img.shields.io/badge/arXiv-2512.07730-b31b1b.svg)](https://arxiv.org/abs/2512.07730)

39. **Revisit What You See: Disclose Language Prior in Vision Tokens for LVLM Decoding**
   Cho, Kim
   [![arXiv](https://img.shields.io/badge/arXiv-2506.09522-b31b1b.svg)](https://arxiv.org/abs/2506.09522)

40. **Revisit What You See: Disclose Language Prior in Vision Tokens for Efficient Guided Decoding of LVLMs**
   Cho, Kim
   [![arXiv](https://img.shields.io/badge/arXiv-2506.09522-b31b1b.svg)](https://arxiv.org/abs/2506.09522)

41. **Reducing Hallucinations in Vision-Language Models via Latent Space Steering**
   Liu, Ye, Zou

42. **Re-Align: Aligning Vision Language Models via Retrieval-Augmented Direct Preference Optimization**
   Xing, Wang, Li et al.
   [![arXiv](https://img.shields.io/badge/arXiv-2502.13146-b31b1b.svg)](https://arxiv.org/abs/2502.13146)

43. **RAR: Reversing Visual Attention Re-sinking for Unlocking Potential in Multimodal Large Language Models**
   Anonymous

44. **Qwen Look Again: Guiding Vision-Language Reasoning Models to Re-attention Visual Information**
   Chu, Chen, Wang et al.
   [![arXiv](https://img.shields.io/badge/arXiv-2505.23558-b31b1b.svg)](https://arxiv.org/abs/2505.23558)

45. **Poison as Cure: Visual Noise for Mitigating Object Hallucinations in LVMs**
   Zhang, Tao, Tang et al.
   [![arXiv](https://img.shields.io/badge/arXiv-2501.19164-b31b1b.svg)](https://arxiv.org/abs/2501.19164)

46. **PerturboLLaVA: Reducing Multimodal Hallucinations with Perturbative Visual Training**
   Chen, Liu, Jing et al.
   [![arXiv](https://img.shields.io/badge/arXiv-2503.06486-b31b1b.svg)](https://arxiv.org/abs/2503.06486)

47. **PerturboLLaVA: Reducing Multimodal Hallucinations with Perturbative Visual Training**
   Chen, Liu, Jing et al.
   [![arXiv](https://img.shields.io/badge/arXiv-2503.06486-b31b1b.svg)](https://arxiv.org/abs/2503.06486)

48. **Paying More Attention to Image: A Training-Free Method for Alleviating Hallucination in LVLMs**
   Anonymous
   [Paper](https://link.springer.com/10.1007/978-3-031-73010-8_8)

49. **PAINT: Paying Attention to INformed Tokens to Mitigate Hallucination in Large Vision-Language Model**
   Arif, Dip, Hussain et al.
   [![arXiv](https://img.shields.io/badge/arXiv-2501.12206-b31b1b.svg)](https://arxiv.org/abs/2501.12206)

50. **Octopus: Alleviating Hallucination via Dynamic Contrastive Decoding**
   Suo, Zhang, Sun et al.
   [![arXiv](https://img.shields.io/badge/arXiv-2503.00361-b31b1b.svg)](https://arxiv.org/abs/2503.00361)

51. **ONLY: One-Layer Intervention Sufficiently Mitigates Hallucinations in Large Vision-Language Models**
   Wan, Zhang, Yong et al.
   [![arXiv](https://img.shields.io/badge/arXiv-2507.00898-b31b1b.svg)](https://arxiv.org/abs/2507.00898)

52. **ONLY: One-Layer Intervention Sufficiently Mitigates Hallucinations in Large Vision-Language Models**
   Wan, Zhang, Yong et al.
   [![arXiv](https://img.shields.io/badge/arXiv-2507.00898-b31b1b.svg)](https://arxiv.org/abs/2507.00898)

53. **Not All Tokens and Heads Are Equally Important: Dual-Level Attention Intervention for Hallucination Mitigation**
   Tang, Zhuang, Yang et al.
   [![arXiv](https://img.shields.io/badge/arXiv-2506.12609-b31b1b.svg)](https://arxiv.org/abs/2506.12609)

54. **Mixture of Decoding: An Attention-Inspired Adaptive Decoding Strategy to Mitigate Hallucinations in Large Vision-Language Models**
   Chen, Zhang, Liu et al.
   [![arXiv](https://img.shields.io/badge/arXiv-2505.17061-b31b1b.svg)](https://arxiv.org/abs/2505.17061)

55. **Mitigating Object Hallucinations via Sentence-Level Early Intervention**
   Peng, Yang, Jiang et al.
   [![arXiv](https://img.shields.io/badge/arXiv-2507.12455-b31b1b.svg)](https://arxiv.org/abs/2507.12455)

56. **Mitigating Object Hallucinations in MLLMs via Multi-Frequency Perturbations**
   Li, Sun, Zheng et al.
   [![arXiv](https://img.shields.io/badge/arXiv-2503.14895-b31b1b.svg)](https://arxiv.org/abs/2503.14895)

57. **Mitigating Object Hallucinations in Large Vision-Language Models via Attention Calibration**
   Zhu, Tao, Dong et al.
   [![arXiv](https://img.shields.io/badge/arXiv-2502.01969-b31b1b.svg)](https://arxiv.org/abs/2502.01969)

58. **Mitigating Object Hallucination in Large Vision-Language Models via Image-Grounded Guidance**
   Zhao, Deng, Zhang et al.
   [![arXiv](https://img.shields.io/badge/arXiv-2402.08680-b31b1b.svg)](https://arxiv.org/abs/2402.08680)

59. **Mitigating Modality Prior-Induced Hallucinations in Multimodal Large Language Models via Deciphering Attention Causality**
   Zhou, Yan, Zou et al.
   [![arXiv](https://img.shields.io/badge/arXiv-2410.04780-b31b1b.svg)](https://arxiv.org/abs/2410.04780)

60. **Mitigating Hallucinations in Large Vision-Language Models via Reasoning Uncertainty-Guided Refinement**
   Li, Xu, Meng et al.
   [Paper](https://ieeexplore.ieee.org/document/11125489/)

61. **Mitigating Hallucinations in Large Vision-Language Models via Entity-Centric Multimodal Preference Optimization**
   Wu, Shi, Wang et al.
   [![arXiv](https://img.shields.io/badge/arXiv-2506.04039-b31b1b.svg)](https://arxiv.org/abs/2506.04039)

62. **Mitigating Hallucination in Multimodal LLMs with Layer Contrastive Decoding**
   Tong, Xia, Zhou
   [![arXiv](https://img.shields.io/badge/arXiv-2509.25177-b31b1b.svg)](https://arxiv.org/abs/2509.25177)

63. **Mitigating Hallucination in Large Vision-Language Models via Adaptive Attention Calibration**
   Fazli, Wei, Sari et al.
   [![arXiv](https://img.shields.io/badge/arXiv-2505.21472-b31b1b.svg)](https://arxiv.org/abs/2505.21472)

64. **Mitigating Hallucination for Large Vision Language Model by Inter-Modality Correlation Calibration Decoding**
   Li, Zhang, Jie et al.
   [![arXiv](https://img.shields.io/badge/arXiv-2501.01926-b31b1b.svg)](https://arxiv.org/abs/2501.01926)

65. **Mitigating Attention Sinks and Massive Activations in Audio-Visual Speech Recognition with LLMs**
   Anand, Cappellazzo, Petridis et al.
   [![arXiv](https://img.shields.io/badge/arXiv-2510.22603-b31b1b.svg)](https://arxiv.org/abs/2510.22603)

66. **Mitigate Language Priors in Large Vision-Language Models by Cross-Images Contrastive Decoding**
   Zhao, Zhang, Sun et al.
   [![arXiv](https://img.shields.io/badge/arXiv-2505.10634-b31b1b.svg)](https://arxiv.org/abs/2505.10634)

67. **MaskCD: Mitigating LVLM Hallucinations by Image Head Masked Contrastive Decoding**
   Deng, Yang
   [![arXiv](https://img.shields.io/badge/arXiv-2510.02790-b31b1b.svg)](https://arxiv.org/abs/2510.02790)

68. **MLLM can see? Dynamic Correction Decoding for Hallucination Mitigation**
   Wang, Chen, Zhang et al.
   [![arXiv](https://img.shields.io/badge/arXiv-2410.11779-b31b1b.svg)](https://arxiv.org/abs/2410.11779)

69. **MINT: Mitigating Hallucinations in Large Vision-Language Models via Token Reduction**
   Wang, Yang, Zhou
   [![arXiv](https://img.shields.io/badge/arXiv-2502.00717-b31b1b.svg)](https://arxiv.org/abs/2502.00717)

70. **MIHBench: Benchmarking and Mitigating Multi-Image Hallucinations in Multimodal Large Language Models**
   Li, Wu, Jin et al.
   [![arXiv](https://img.shields.io/badge/arXiv-2508.00726-b31b1b.svg)](https://arxiv.org/abs/2508.00726)

71. **MCA-LLaVA: Manhattan Causal Attention for Reducing Hallucination in Large Vision-Language Models**
   Zhao, Zhang, Li et al.
   [![arXiv](https://img.shields.io/badge/arXiv-2507.09184-b31b1b.svg)](https://arxiv.org/abs/2507.09184)

72. **MAP: Mitigating Hallucinations in Large Vision-Language Models with Map-Level Attention Processing**
   Li, Guo, Qian et al.
   [![arXiv](https://img.shields.io/badge/arXiv-2508.01653-b31b1b.svg)](https://arxiv.org/abs/2508.01653)

73. **Look Twice Before You Answer: Memory-Space Visual Retracing for Hallucination Mitigation in Multimodal Large Language Models**
   Zou, Wang, Yan et al.
   [![arXiv](https://img.shields.io/badge/arXiv-2410.03577-b31b1b.svg)](https://arxiv.org/abs/2410.03577)

74. **Intervening Anchor Token: Decoding Strategy in Alleviating Hallucinations for MLLMs**
   Tang, Huang, Liu et al.

75. **Interpreting and Editing Vision-Language Representations to Mitigate Hallucinations**
   Jiang, Kachinthaya, Petyrk et al.

76. **Insight Over Sight: Exploring the Vision-Knowledge Conflicts in Multimodal LLMs**
   Liu, Wang, Yuan et al.
   [![arXiv](https://img.shields.io/badge/arXiv-2410.08145-b31b1b.svg)](https://arxiv.org/abs/2410.08145)

77. **InEx: Hallucination Mitigation via Introspection and Cross-Modal Multi-Agent Collaboration**
   Yang, Yuan, Jiang et al.
   [![arXiv](https://img.shields.io/badge/arXiv-2512.02981-b31b1b.svg)](https://arxiv.org/abs/2512.02981)

78. **INTER: Mitigating Hallucination in Large Vision-Language Models by Interaction Guidance Sampling**
   Dong, Dong, Wang et al.
   [![arXiv](https://img.shields.io/badge/arXiv-2507.05056-b31b1b.svg)](https://arxiv.org/abs/2507.05056)

79. **Hallucinatory Image Tokens: A Training-free EAZY Approach on Detecting and Mitigating Object Hallucinations in LVLMs**
   Che, Liu, Jia et al.
   [![arXiv](https://img.shields.io/badge/arXiv-2503.07772-b31b1b.svg)](https://arxiv.org/abs/2503.07772)

80. **Grounding Language with Vision: A Conditional Mutual Information Calibrated Decoding Strategy for Reducing Hallucinations in LVLMs**
   Fang, Zhou, Kong et al.
   [![arXiv](https://img.shields.io/badge/arXiv-2505.19678-b31b1b.svg)](https://arxiv.org/abs/2505.19678)

81. **GHOST: Hallucination-Inducing Image Generation for Multimodal LLMs**
   Parast, Hosseini, Asadollahzadeh et al.
   [![arXiv](https://img.shields.io/badge/arXiv-2509.25178-b31b1b.svg)](https://arxiv.org/abs/2509.25178)

82. **Focus on What Matters: Enhancing Medical Vision-Language Models with Automatic Attention Alignment Tuning**
   Chang, Huang, Boyd et al.
   [![arXiv](https://img.shields.io/badge/arXiv-2505.18503-b31b1b.svg)](https://arxiv.org/abs/2505.18503)

83. **Extracting Visual Facts from Intermediate Layers for Mitigating Hallucinations in Multimodal Large Language Models**
   Zhou, Zhang, Chen
   [![arXiv](https://img.shields.io/badge/arXiv-2507.15652-b31b1b.svg)](https://arxiv.org/abs/2507.15652)

84. **Enhancing Visual Reliance in Text Generation: A Bayesian Perspective on Mitigating Hallucination in Large Vision-Language Models**
   Hu, Duan, Zhang et al.
   [![arXiv](https://img.shields.io/badge/arXiv-2505.19498-b31b1b.svg)](https://arxiv.org/abs/2505.19498)

85. **Energy-Guided Decoding for Object Hallucination Mitigation**
   Liu, Deng, Zach
   [![arXiv](https://img.shields.io/badge/arXiv-2507.07731-b31b1b.svg)](https://arxiv.org/abs/2507.07731)

86. **Don't Miss the Forest for the Trees: Attentional Vision Calibration for Large Vision Language Models**
   Woo, Kim, Jang et al.
   [![arXiv](https://img.shields.io/badge/arXiv-2405.17820-b31b1b.svg)](https://arxiv.org/abs/2405.17820)

87. **Don't Deceive Me: Mitigating Gaslighting through Attention Reallocation in LMMs**
   Jiao, Zhu, Chen et al.
   [![arXiv](https://img.shields.io/badge/arXiv-2504.09456-b31b1b.svg)](https://arxiv.org/abs/2504.09456)

88. **Devils in Middle Layers of Large Vision-Language Models: Interpreting, Detecting and Mitigating Object Hallucinations via Attention Lens**
   Jiang, Chen, Zhu et al.
   [![arXiv](https://img.shields.io/badge/arXiv-2411.16724-b31b1b.svg)](https://arxiv.org/abs/2411.16724)

89. **Decoupling Contrastive Decoding: Robust Hallucination Mitigation in Multimodal Large Language Models**
   Chen, Yan, Wen et al.
   [![arXiv](https://img.shields.io/badge/arXiv-2504.08809-b31b1b.svg)](https://arxiv.org/abs/2504.08809)

90. **Damo: Decoding by Accumulating Activations Momentum for Mitigating Hallucinations in Vision-Language Models**
   Wang, Gu, Gao et al.

91. **D-LEAF: Localizing and Correcting Hallucinations in Multimodal LLMs via Layer-to-head Attention Diagnostics**
   Yang, Zhang, Lin et al.
   [![arXiv](https://img.shields.io/badge/arXiv-2509.07864-b31b1b.svg)](https://arxiv.org/abs/2509.07864)

92. **Cross-Image Contrastive Decoding: Precise, Lossless Suppression of Language Priors in Large Vision-Language Models**
   Zhao, Zhang, Sun et al.
   [![arXiv](https://img.shields.io/badge/arXiv-2505.10634-b31b1b.svg)](https://arxiv.org/abs/2505.10634)

93. **Cracking the Code of Hallucination in LVLMs with Vision-aware Head Divergence**
   He, Zhu, Guo et al.
   [![arXiv](https://img.shields.io/badge/arXiv-2412.13949-b31b1b.svg)](https://arxiv.org/abs/2412.13949)

94. **Conscious Gaze: Adaptive Attention Mechanisms for Hallucination Mitigation in Vision-Language Models**
   Bu, Yuan, Zhang
   [![arXiv](https://img.shields.io/badge/arXiv-2512.05546-b31b1b.svg)](https://arxiv.org/abs/2512.05546)

95. **CoFi-Dec: Hallucination-Resistant Decoding via Coarse-to-Fine Generative Feedback in Large Vision-Language Models**
   Cao, He, Liu et al.
   [![arXiv](https://img.shields.io/badge/arXiv-2512.23453-b31b1b.svg)](https://arxiv.org/abs/2512.23453)

96. **Causally-Grounded Dual-Path Attention Intervention for Object Hallucination Mitigation in LVLMs**
   Yu, Chen, Kuang et al.
   [![arXiv](https://img.shields.io/badge/arXiv-2511.09018-b31b1b.svg)](https://arxiv.org/abs/2511.09018)

97. **Causally-Grounded Dual-Path Attention Intervention for Object Hallucination Mitigation in LVLMs**
   Yu, Chen, Kuang et al.
   [![arXiv](https://img.shields.io/badge/arXiv-2511.09018-b31b1b.svg)](https://arxiv.org/abs/2511.09018)

98. **Causal Tracing of Object Representations in Large Vision Language Models: Mechanistic Interpretability and Hallucination Mitigation**
   Li, Ye, Feng et al.
   [![arXiv](https://img.shields.io/badge/arXiv-2511.05923-b31b1b.svg)](https://arxiv.org/abs/2511.05923)

99. **Capturing Gaze Shifts for Guidance: Cross-Modal Fusion Enhancement for VLM Hallucination Mitigation**
   Qi, Shang, Spiliopoulou et al.
   [![arXiv](https://img.shields.io/badge/arXiv-2510.22067-b31b1b.svg)](https://arxiv.org/abs/2510.22067)

100. **CLAIM: Mitigating Multilingual Object Hallucination in Large Vision-Language Models with Cross-Lingual Attention Intervention**
   Ye, Li, Feng et al.
   [![arXiv](https://img.shields.io/badge/arXiv-2506.11073-b31b1b.svg)](https://arxiv.org/abs/2506.11073)

101. **CAI: Caption-Sensitive Attention Intervention for Mitigating Object Hallucination in Large Vision-Language Models**
   Li, Ye, Feng et al.
   [![arXiv](https://img.shields.io/badge/arXiv-2506.23590-b31b1b.svg)](https://arxiv.org/abs/2506.23590)

102. **Beyond Single Models: Mitigating Multimodal Hallucinations via Adaptive Token Ensemble Decoding**
   Li, Wang, Yuan et al.
   [![arXiv](https://img.shields.io/badge/arXiv-2510.18321-b31b1b.svg)](https://arxiv.org/abs/2510.18321)

103. **Benchmarking and Bridging Emotion Conflicts for Multimodal Emotion Reasoning**
   Han, Zhu, Xu et al.
   [![arXiv](https://img.shields.io/badge/arXiv-2508.01181-b31b1b.svg)](https://arxiv.org/abs/2508.01181)

104. **Attention Hijackers: Detect and Disentangle Attention Hijacking in LVLMs for Hallucination Mitigation**
   Chen, Lyu, Gao et al.
   [![arXiv](https://img.shields.io/badge/arXiv-2503.08216-b31b1b.svg)](https://arxiv.org/abs/2503.08216)

105. **Analyzing and Mitigating Object Hallucination: A Training Bias Perspective**
   Li, Zhou, Zhao et al.
   [![arXiv](https://img.shields.io/badge/arXiv-2508.04567-b31b1b.svg)](https://arxiv.org/abs/2508.04567)

106. **Adaptive Residual-Update Steering for Low-Overhead Hallucination Mitigation in Large Vision Language Models**
   Zou, Gao, Guan et al.
   [![arXiv](https://img.shields.io/badge/arXiv-2511.10292-b31b1b.svg)](https://arxiv.org/abs/2511.10292)

107. **AVCD: Mitigating Hallucinations in Audio-Visual Large Language Models through Contrastive Decoding**
   Jung, Jang, Chung
   [![arXiv](https://img.shields.io/badge/arXiv-2505.20862-b31b1b.svg)](https://arxiv.org/abs/2505.20862)

108. **ASCD: Attention-Steerable Contrastive Decoding for Reducing Hallucination in MLLM**
   Wang, Bi, Ma et al.
   [![arXiv](https://img.shields.io/badge/arXiv-2506.14766-b31b1b.svg)](https://arxiv.org/abs/2506.14766)

109. **A Survey of Multimodal Hallucination Evaluation and Detection**
   Chen, Min, Zhang et al.
   [![arXiv](https://img.shields.io/badge/arXiv-2507.19024-b31b1b.svg)](https://arxiv.org/abs/2507.19024)


## 2024

110. **Visual Evidence Prompting Mitigates Hallucinations in Large Vision-Language Models**
   Li, Huang, Li et al.

111. **Verb Mirage: Unveiling and Assessing Verb Concept Hallucinations in Multimodal Large Language Models**
   Wang, Liu, Wu et al.
   [![arXiv](https://img.shields.io/badge/arXiv-2412.04939-b31b1b.svg)](https://arxiv.org/abs/2412.04939)

112. **VaLiD: Mitigating the Hallucination of Large Vision Language Models by Visual Layer Fusion Contrastive Decoding**
   Wang, Gao, Sang
   [![arXiv](https://img.shields.io/badge/arXiv-2411.15839-b31b1b.svg)](https://arxiv.org/abs/2411.15839)

113. **VISTA: Enhancing Vision-Text Alignment in MLLMs via Cross- Modal Mutual Information Maximization**
   Li, Su, Qu et al.

114. **VASparse: Towards Efficient Visual Hallucination Mitigation via Visual-Aware Token Sparsification**
   Zhuang, Zhu, Xie et al.

115. **VACoDe: Visual Augmented Contrastive Decoding**
   Kim, Cho, Bae et al.
   [![arXiv](https://img.shields.io/badge/arXiv-2408.05337-b31b1b.svg)](https://arxiv.org/abs/2408.05337)

116. **Unraveling Cross-Modality Knowledge Conflict in Large Vision-Language Models**
   Zhu, Liu, Wang et al.
   [![arXiv](https://img.shields.io/badge/arXiv-2410.03659-b31b1b.svg)](https://arxiv.org/abs/2410.03659)

117. **Shallow Focus, Deep Fixes: Enhancing Shallow Layers Vision Attention Sinks to Alleviate Hallucination in LVLMs**
   Zhang, Quan, Shen et al.

118. **Seeing the Image: Prioritizing Visual Correlation by Contrastive Alignment**
   Xiao, Wu, Wang et al.
   [![arXiv](https://img.shields.io/badge/arXiv-2405.17871-b31b1b.svg)](https://arxiv.org/abs/2405.17871)

119. **Seeing Far and Clearly: Mitigating Hallucinations in MLLMs with Attention Causal Decoding**
   Tang, Liu, Xu et al.

120. **Seeing Clearly by Layer Two: Enhancing Attention Heads to Alleviate Hallucination in LVLMs**
   Zhang, Quan, Gu et al.
   [![arXiv](https://img.shields.io/badge/arXiv-2411.09968-b31b1b.svg)](https://arxiv.org/abs/2411.09968)

121. **RITUAL: Random Image Transformations as a Universal Anti-hallucination Lever in Large Vision Language Models**
   Woo, Jang, Kim et al.
   [![arXiv](https://img.shields.io/badge/arXiv-2405.17821-b31b1b.svg)](https://arxiv.org/abs/2405.17821)

122. **OPERA: Alleviating Hallucination in Multi-Modal Large Language Models via Over-Trust Penalty and Retrospection-Allocation**
   Huang, Dong, Zhang et al.
   [Paper](https://ieeexplore.ieee.org/document/10655465/)

123. **Nullu: Mitigating Object Hallucinations in Large Vision-Language Models via HalluSpace Projection**
   Yang, Zheng, Chen et al.

124. **Multi-Modal Hallucination Control by Visual Information Grounding**
   Favero, Zancato, Trager et al.
   [Paper](https://ieeexplore.ieee.org/document/10655750/)

125. **MoLE: Decoding by Mixture of Layer Experts Alleviates Hallucination in Large Vision-Language Models**
   Liang, Du, Huang et al.

126. **Mitigating Object Hallucinations in Large Vision-Language Models with Assembly of Global and Local Attention**
   An, Tian, Leng et al.

127. **Mitigating Object Hallucinations in Large Vision-Language Models through Visual Contrastive Decoding**
   Leng, Zhang, Chen et al.

128. **Mitigating Object Hallucination via Concentric Causal Attention**
   Xing, Li, Laptev et al.

129. **Mitigating Multilingual Hallucination in Large Vision-Language Models**
   Qu, Song, Wei et al.
   [![arXiv](https://img.shields.io/badge/arXiv-2408.00550-b31b1b.svg)](https://arxiv.org/abs/2408.00550)

130. **Mitigating Hallucinations in Multi-modal Large Language Models via Image Token Attention-Guided Decoding**
   Xu, Chen, Lyu et al.

131. **Mitigating Hallucinations in Large Vision-Language Models with Instruction Contrastive Decoding**
   Wang, Pan, Ding et al.
   [![arXiv](https://img.shields.io/badge/arXiv-2403.18715-b31b1b.svg)](https://arxiv.org/abs/2403.18715)

132. **Mitigating Hallucinations in Large Vision-Language Models by Adaptively Constraining Information Flow**
   Bai, Guo, Peng et al.

133. **Layer Importance and Hallucination Analysis in Large Language Models via Enhanced Activation Variance-Sparsity**
   Song, Huang, Wu et al.
   [![arXiv](https://img.shields.io/badge/arXiv-2411.10069-b31b1b.svg)](https://arxiv.org/abs/2411.10069)

134. **Instructive Decoding: Instruction-Tuned Large Language Models are Self-Refiner from Noisy Instructions**
   Kim, Kim, Lee et al.
   [![arXiv](https://img.shields.io/badge/arXiv-2311.00233-b31b1b.svg)](https://arxiv.org/abs/2311.00233)

135. **In-Context Sharpness as Alerts: An Inner Representation Perspective for Hallucination Mitigation**
   Chen, Xiong, Liu et al.
   [![arXiv](https://img.shields.io/badge/arXiv-2403.01548-b31b1b.svg)](https://arxiv.org/abs/2403.01548)

136. **Improve Decoding Factuality by Token-wise Cross Layer Entropy of Large Language Models**
   Wu, Shen, Liu et al.

137. **IBD: Alleviating Hallucinations in Large Vision-Language Models via Image-Biased Decoding**
   Zhu, Ji, Chen et al.
   [![arXiv](https://img.shields.io/badge/arXiv-2402.18476-b31b1b.svg)](https://arxiv.org/abs/2402.18476)

138. **HALC: Object Hallucination Reduction via Adaptive Focal-Contrast Decoding**
   Chen, Zhao, Luo et al.
   [![arXiv](https://img.shields.io/badge/arXiv-2403.00425-b31b1b.svg)](https://arxiv.org/abs/2403.00425)

139. **DoLa: Decoding by Contrasting Layers Improves Factuality in Large Language Models**
   Chuang, Xie, Luo et al.
   [![arXiv](https://img.shields.io/badge/arXiv-2309.03883-b31b1b.svg)](https://arxiv.org/abs/2309.03883)

140. **DAMRO: Dive into the Attention Mechanism of LVLM to Reduce Object Hallucination**
   Gong, Ming, Wang et al.
   [Paper](https://aclanthology.org/2024.emnlp-main.439)

141. **Contrastive Region Guidance: Improving Grounding in Vision-Language Models without Training**
   Wan, Cho, Stengel-Eskin et al.
   [![arXiv](https://img.shields.io/badge/arXiv-2403.02325-b31b1b.svg)](https://arxiv.org/abs/2403.02325)

142. **Contrastive Decoding Reduces Hallucinations in Large Multilingual Machine Translation Models**
   Waldendorf, Haddow, Birch

143. **ConVis: Contrastive Decoding with Hallucination Visualization for Mitigating Hallucinations in Multimodal Large Language Models**
   Park, Lee, Choe et al.
   [![arXiv](https://img.shields.io/badge/arXiv-2408.13906-b31b1b.svg)](https://arxiv.org/abs/2408.13906)

144. **CODE: Contrasting Self-generated Description to Combat Hallucination in Large Multi-modal Models**
   Kim, Kim, Kim et al.
   [![arXiv](https://img.shields.io/badge/arXiv-2406.01920-b31b1b.svg)](https://arxiv.org/abs/2406.01920)

145. **BIMA: Bijective Maximum Likelihood Learning Approach to Hallucination Prediction and Mitigation in Large Vision-Language Models**
   Tran, Truong, Luu

146. **Analyzing and Mitigating Object Hallucination in Large Vision-Language Models**
   Zhou, Cui, Yoon et al.
   [![arXiv](https://img.shields.io/badge/arXiv-2310.00754-b31b1b.svg)](https://arxiv.org/abs/2310.00754)

147. **Alleviating Hallucinations in Large Vision-Language Models through Hallucination-Induced Optimization**
   Chen, Lyu, Gao et al.
   [![arXiv](https://img.shields.io/badge/arXiv-2405.15356-b31b1b.svg)](https://arxiv.org/abs/2405.15356)

148. **Alleviating Hallucination in Large Vision-Language Models with Active Retrieval Augmentation**
   Qu, Chen, Wei et al.

149. **Activation Steering Decoding: Mitigating Hallucination in Large Vision-Language Models through Bidirectional Hidden State Intervention**
   Su, Chen, Li et al.


## 2023

150. **Mitigating Object Hallucinations in Large Vision-Language Models through Visual Contrastive Decoding**
   Leng, Zhang, Chen et al.
   [![arXiv](https://img.shields.io/badge/arXiv-2311.16922-b31b1b.svg)](https://arxiv.org/abs/2311.16922)

151. **Evaluating Object Hallucination in Large Vision-Language Models**
   Li, Du, Zhou et al.
   [![arXiv](https://img.shields.io/badge/arXiv-2305.10355-b31b1b.svg)](https://arxiv.org/abs/2305.10355)

152. **Contrastive Decoding: Open-ended Text Generation as Optimization**
   Li, Holtzman, Fried et al.
   [![arXiv](https://img.shields.io/badge/arXiv-2210.15097-b31b1b.svg)](https://arxiv.org/abs/2210.15097)

